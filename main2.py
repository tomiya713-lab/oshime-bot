# -*- coding: utf-8 -*-
# main.py â€” æŠ¼ã—ç›®æŠ½å‡º â†’ LINEã¸ã€Œ4è¡Œãƒ†ã‚­ã‚¹ãƒˆâ†’ãƒãƒ£ãƒ¼ãƒˆç”»åƒã€é€£ç¶šé€ä¿¡ï¼ˆMessaging APIï¼‰
# ä¾å­˜: pandas, numpy, yfinance, mplfinance, requests
# ç’°å¢ƒå¤‰æ•°:
#   LINE_CHANNEL_ACCESS_TOKEN, LINE_USER_ID, PUBLIC_BASE_URL
#   ï¼ˆä»»æ„ï¼‰FORCE_RUN=1 ã§é€±æœ«ã‚¹ã‚­ãƒƒãƒ—ç„¡åŠ¹åŒ–
#   ï¼ˆä»»æ„ï¼‰TICKERS_CSV=./tickers.csv  (Tickeråˆ—ã‚’å«ã‚€CSV)
#   ï¼ˆä»»æ„ï¼‰LOOKBACK_DAYS=180
#
# GitHub Actionsã§ charts/ ã‚’ GitHub Pages ã«å…¬é–‹ã—ã€PUBLIC_BASE_URL/charts/<TICKER>.png ã‚’é€ã‚Šã¾ã™ã€‚

import os
import sys
import math
import json
from datetime import datetime, timedelta
import requests
import numpy as np
import pandas as pd
import yfinance as yf
import mplfinance as mpf

# ===== è¨­å®šï¼ˆå¿…è¦ã«å¿œã˜ã¦å¤‰æ›´ï¼‰ =====
TZ_OFFSET = 9  # JST
REBOUND_MIN = 2.0       # åç™ºç‡ >= 2%
REBOUND_MAX = 4.0       # åç™ºç‡ <= 4%
DROP_MAX = 15.0         # ãƒ”ãƒ¼ã‚¯ã‹ã‚‰ã®è¨±å®¹ä¸‹è½ç‡ <= 15%
DAYS_SINCE_MIN = 2      # æŠ¼ã—ç›®ã‹ã‚‰æœ€æ–°ã¾ã§ã®å–¶æ¥­æ—¥æ•° >= 2
EXPECTED_RISE_MIN = 3.0 # æœŸå¾…ä¸Šæ˜‡ç‡ >= 3%
SMA_WINDOW = 25
TOP_N = 15              # é€ä¿¡ä¸Šé™ï¼ˆå¤šã™ãã‚‹ã¨è¦‹ã¥ã‚‰ã„ã®ã§é©åº¦ã«ï¼‰
DEFAULT_LOOKBACK_DAYS = int(os.getenv("LOOKBACK_DAYS", "180"))

# ===== LINE (Messaging API) å¿…é ˆ =====
LINE_CHANNEL_ACCESS_TOKEN = os.environ.get("LINE_CHANNEL_ACCESS_TOKEN", "")
LINE_USER_ID = os.environ.get("LINE_USER_ID", "")
PUBLIC_BASE_URL = os.environ.get("PUBLIC_BASE_URL", "").rstrip("/")

if not LINE_CHANNEL_ACCESS_TOKEN or not LINE_USER_ID:
    print("[ERROR] LINE env is missing. Set LINE_CHANNEL_ACCESS_TOKEN and LINE_USER_ID.", file=sys.stderr)

# ===== ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ =====
def now_jst():
    return datetime.utcnow() + timedelta(hours=TZ_OFFSET)

def is_weekend(dt: datetime) -> bool:
    # åœŸæ—¥ã‚¹ã‚­ãƒƒãƒ—ï¼ˆæ—¥æœ¬ã®ç¥æ—¥ã¯è€ƒæ…®ã—ãªã„ã€‚å¿…è¦ãªã‚‰æ‹¡å¼µï¼‰
    return dt.weekday() >= 5

def chunk_text(text: str, limit: int = 4500):
    # LINEã®ãƒ†ã‚­ã‚¹ãƒˆã¯æœ€å¤§5000æ–‡å­—ã€‚ä½™è£•ã‚’ã¿ã¦åˆ†å‰²
    out = []
    buf = []
    size = 0
    for line in text.splitlines():
        if size + len(line) + 1 > limit:
            out.append("\n".join(buf))
            buf, size = [line], len(line) + 1
        else:
            buf.append(line)
            size += len(line) + 1
    if buf:
        out.append("\n".join(buf))
    return out

# ===== LINEé€ä¿¡ =====
def line_push_text(msg: str, to_user_id: str | None = None):
    uid = to_user_id or LINE_USER_ID
    headers = {"Authorization": f"Bearer {LINE_CHANNEL_ACCESS_TOKEN}", "Content-Type": "application/json"}
    for part in chunk_text(msg):
        payload = {"to": uid, "messages": [{"type": "text", "text": part}]}
        r = requests.post("https://api.line.me/v2/bot/message/push", headers=headers, json=payload, timeout=20)
        if r.status_code >= 300:
            raise RuntimeError(f"LINE text send failed: {r.status_code} {r.text}")

def line_push_image(public_url: str, to_user_id: str | None = None):
    if not PUBLIC_BASE_URL:
        print("[WARN] PUBLIC_BASE_URL is not set. Image message will be skipped.", file=sys.stderr)
        return
    uid = to_user_id or LINE_USER_ID
    headers = {"Authorization": f"Bearer {LINE_CHANNEL_ACCESS_TOKEN}", "Content-Type": "application/json"}
    payload = {
        "to": uid,
        "messages": [{
            "type": "image",
            "originalContentUrl": public_url,
            "previewImageUrl": public_url
        }]
    }
    r = requests.post("https://api.line.me/v2/bot/message/push", headers=headers, json=payload, timeout=20)
    if r.status_code >= 300:
        raise RuntimeError(f"LINE image send failed: {r.status_code} {r.text}")

def send_long_text(msg: str):
    # é•·æ–‡ã‚’è‡ªå‹•åˆ†å‰²ã—ã¦é€ã‚‹
    for part in chunk_text(msg):
        line_push_text(part)

# ===== ãƒ‡ãƒ¼ã‚¿å–å¾— =====
def load_tickers():
    # å„ªå…ˆ: ç’°å¢ƒå¤‰æ•° TICKERS_CSV ã®CSVï¼ˆTickeråˆ—ï¼‰
    csv_path = os.getenv("TICKERS_CSV")
    if csv_path and os.path.exists(csv_path):
        df = pd.read_csv(csv_path)
        col = None
        for c in df.columns:
            if c.lower() in ("ticker", "symbol", "code"):
                col = c
                break
        if col:
            tickers = [str(x).strip() for x in df[col].dropna().unique().tolist()]
            if tickers:
                return tickers

    return nikkei225_tickers

# ===== æ—¥çµŒ225ãƒ†ã‚£ãƒƒã‚«ãƒ¼ =====
nikkei225_tickers = [ '4151.T','4502.T','4503.T','4506.T','4507.T','4519.T','4523.T','4568.T','4578.T','6479.T','6501.T','6503.T','6504.T','6506.T','6526.T','6594.T','6645.T','6674.T','6701.T','6702.T','6723.T','6724.T','6752.T','6753.T','6758.T','6762.T','6770.T','6841.T','6857.T','6861.T','6902.T','6920.T','6952.T','6954.T','6971.T','6976.T','6981.T','7735.T','7751.T','7752.T','8035.T','7201.T','7202.T','7203.T','7205.T','7211.T','7261.T','7267.T','7269.T','7270.T','7272.T','4543.T','4902.T','6146.T','7731.T','7733.T','7741.T','7762.T','9432.T','9433.T','9434.T','9613.T','9984.T','5831.T','7186.T','8304.T','8306.T','8308.T','8309.T','8316.T','8331.T','8354.T','8411.T','8253.T','8591.T','8697.T','8601.T','8604.T','8630.T','8725.T','8750.T','8766.T','8795.T','1332.T','2002.T','2269.T','2282.T','2501.T','2502.T','2503.T','2801.T','2802.T','2871.T','2914.T','3086.T','3092.T','3099.T','3382.T','7453.T','8233.T','8252.T','8267.T','9843.T','9983.T','2413.T','2432.T','3659.T','4307.T','4324.T','4385.T','4661.T','4689.T','4704.T','4751.T','4755.T','6098.T','6178.T','7974.T','9602.T','9735.T','9766.T','1605.T','3401.T','3402.T','3861.T','3405.T','3407.T','4004.T','4005.T','4021.T','4042.T','4043.T','4061.T','4063.T','4183.T','4188.T','4208.T','4452.T','4901.T','4911.T','6988.T','5019.T','5020.T','5101.T','5108.T','5201.T','5214.T','5233.T','5301.T','5332.T','5333.T','5401.T','5406.T','5411.T','3436.T','5706.T','5711.T','5713.T','5714.T','5801.T','5802.T','5803.T','2768.T','8001.T','8002.T','8015.T','8031.T','8053.T','8058.T','1721.T','1801.T','1802.T','1803.T','1808.T','1812.T','1925.T','1928.T','1963.T','5631.T','6103.T','6113.T','6273.T','6301.T','6302.T','6305.T','6326.T','6361.T','6367.T','6471.T','6472.T','6473.T','7004.T','7011.T','7013.T','7012.T','7832.T','7911.T','7912.T','7951.T','3289.T','8801.T','8802.T','8804.T','8830.T','9001.T','9005.T','9007.T','9008.T','9009.T','9020.T','9021.T','9022.T','9064.T','9147.T','9101.T','9104.T','9107.T','9201.T','9202.T','9301.T','9501.T','9502.T','9503.T','9531.T','9532.T' ]

# ===== çŸ­ç¸®åãƒãƒƒãƒ— =====
ticker_name_map = {
    "1332.T": "æ—¥æ°´", "1333.T": "ãƒãƒ«ãƒãƒ‹ãƒãƒ­", "1605.T": "INPEX", "1801.T": "å¤§æˆå»º",
    "1802.T": "æ¸…æ°´å»º", "1803.T": "é£›å³¶å»º", "1808.T": "é•·è°·å·¥", "1812.T": "é¹¿å³¶",
    "1925.T": "å¤§å’Œãƒã‚¦ã‚¹", "1928.T": "ç©æ°´ãƒã‚¦ã‚¹", "1963.T": "æ—¥æ®HD", "2002.T": "æ—¥æ¸…ç²‰G",
    "2269.T": "æ˜æ²»HD", "2282.T": "æ—¥æœ¬ãƒãƒ ", "2413.T": "ã‚¨ãƒ ã‚¹ãƒªãƒ¼", "2432.T": "DeNA",
    "2501.T": "ã‚µãƒƒãƒãƒ­HD", "2502.T": "ã‚¢ã‚µãƒ’GHD", "2503.T": "ã‚­ãƒªãƒ³HD", "2768.T": "åŒæ—¥",
    "2801.T": "ã‚­ãƒƒã‚³ãƒãƒ³", "2802.T": "å‘³ã®ç´ ", "2871.T": "ãƒ‹ãƒãƒ¬ã‚¤", "2914.T": "JT",
    "3086.T": "Jãƒ•ãƒ­ãƒ³ãƒˆ", "3092.T": "ZOZO", "3099.T": "ä¸‰è¶Šä¼Šå‹¢ä¸¹", "3382.T": "ã‚»ãƒ–ãƒ³&ã‚¢ã‚¤",
    "3401.T": "å¸äºº", "3402.T": "æ±ãƒ¬", "3405.T": "ã‚¯ãƒ©ãƒ¬", "3407.T": "æ—­åŒ–æˆ",
    "3436.T": "SUMCO", "3861.T": "ç‹å­HD", "4004.T": "æ˜­é›»å·¥", "4005.T": "ä½å‹åŒ–å­¦",
    "4021.T": "æ—¥ç”£åŒ–", "4042.T": "æ±ã‚½ãƒ¼", "4043.T": "ãƒˆã‚¯ãƒ¤ãƒ", "4061.T": "é›»åŒ–",
    "4063.T": "ä¿¡è¶ŠåŒ–", "4183.T": "ä¸‰äº•åŒ–å­¦", "4188.T": "ä¸‰è±ã‚±ãƒŸHD", "4208.T": "UBE",
    "4452.T": "èŠ±ç‹", "4502.T": "æ­¦ç”°è–¬å“", "4503.T": "ã‚¢ã‚¹ãƒ†ãƒ©ã‚¹", "4506.T": "å¤§æ—¥æœ¬ä½å‹",
    "4507.T": "å¡©é‡ç¾©", "4519.T": "ä¸­å¤–è£½è–¬", "4523.T": "ã‚¨ãƒ¼ã‚¶ã‚¤", "4543.T": "ãƒ†ãƒ«ãƒ¢",
    "4568.T": "ç¬¬ä¸€ä¸‰å…±", "4578.T": "å¤§å¡šHD", "4661.T": "OLC", "4689.T": "ZHD",
    "4704.T": "ãƒˆãƒ¬ãƒ³ãƒ‰", "4751.T": "ã‚µã‚¤ãƒãƒ¼", "4755.T": "æ¥½å¤©G", "4901.T": "å¯Œå£«ãƒ•ã‚¤ãƒ«ãƒ ",
    "4902.T": "ã‚³ãƒ‹ã‚«ãƒŸãƒãƒ«ã‚¿", "4911.T": "è³‡ç”Ÿå ‚", "5020.T": "ENEOS",
    "5101.T": "æ¨ªæµœã‚´ãƒ ", "5108.T": "ãƒ–ãƒªãƒ‚ã‚¹ãƒˆãƒ³", "5201.T": "AGC", "5214.T": "æ—¥é›»ç¡",
    "5233.T": "å¤ªå¹³æ´‹ã‚»ãƒ¡", "5301.T": "æ±æµ·ã‚«ãƒ¼ãƒœãƒ³", "5332.T": "TOTO", "5333.T": "æ—¥æœ¬ã‚¬ã‚¤ã‚·",
    "5401.T": "æ—¥æœ¬è£½é‰„", "5406.T": "ç¥æˆ¸è£½é‹¼", "5411.T": "JFEHD", "5706.T": "ä¸‰äº•é‡‘å±",
    "5711.T": "ä¸‰è±ãƒãƒ†", "5713.T": "ä½å‹é‡‘å±é‰±å±±", "5714.T": "DOWA", "5801.T": "å¤æ²³é›»å·¥",
    "5802.T": "ä½å‹é›»å·¥", "5803.T": "ãƒ•ã‚¸ã‚¯ãƒ©", "6098.T": "ãƒªã‚¯ãƒ«ãƒ¼ãƒˆHD", "6178.T": "æ—¥æœ¬éƒµæ”¿",
    "6273.T": "SMC", "6301.T": "ã‚³ãƒãƒ„", "6302.T": "ä½å‹é‡æ©Ÿ", "6305.T": "æ—¥ç«‹å»ºæ©Ÿ",
    "6326.T": "ã‚¯ãƒœã‚¿", "6361.T": "èåŸ", "6367.T": "ãƒ€ã‚¤ã‚­ãƒ³", "6471.T": "æ—¥ç²¾å·¥",
    "6472.T": "NTN", "6473.T": "ã‚¸ã‚§ã‚¤ãƒ†ã‚¯ãƒˆ", "6479.T": "ãƒŸãƒãƒ™ã‚¢ãƒŸãƒ„ãƒŸ", "6501.T": "æ—¥ç«‹",
    "6503.T": "ä¸‰è±é›»æ©Ÿ", "6504.T": "å¯Œå£«é›»æ©Ÿ", "6506.T": "å®‰å·é›»æ©Ÿ", "6526.T": "ã‚½ã‚·ã‚ªãƒã‚¯ã‚¹ãƒˆ",
    "6594.T": "æ—¥é›»ç”£", "6645.T": "ã‚ªãƒ ãƒ­ãƒ³", "6674.T": "ã‚¸ãƒ¼ã‚¨ã‚¹ãƒ¦ã‚¢ã‚µ", "6701.T": "NEC",
    "6702.T": "å¯Œå£«é€š", "6723.T": "ãƒ«ãƒã‚µã‚¹", "6724.T": "ã‚»ã‚¤ã‚³ãƒ¼ã‚¨ãƒ—ã‚½ãƒ³", "6752.T": "ãƒ‘ãƒŠã‚½ãƒ‹ãƒƒã‚¯",
    "6753.T": "ã‚·ãƒ£ãƒ¼ãƒ—", "6758.T": "ã‚½ãƒ‹ãƒ¼G", "6762.T": "TDK", "6770.T": "ã‚¢ãƒ«ãƒ—ã‚¹ã‚¢ãƒ«ãƒ‘",
    "6841.T": "æ¨ªæ²³é›»æ©Ÿ", "6857.T": "ã‚¢ãƒ‰ãƒ†ã‚¹ãƒˆ", "6861.T": "ã‚­ãƒ¼ã‚¨ãƒ³ã‚¹", "6902.T": "ãƒ‡ãƒ³ã‚½ãƒ¼",
    "6920.T": "ãƒ¬ãƒ¼ã‚¶ãƒ¼ãƒ†ãƒƒã‚¯", "6952.T": "ã‚«ã‚·ã‚ª", "6954.T": "ãƒ•ã‚¡ãƒŠãƒƒã‚¯", "6971.T": "äº¬ã‚»ãƒ©",
    "6976.T": "å¤ªé™½èª˜é›»", "6981.T": "æ‘ç”°è£½ä½œæ‰€", "6988.T": "æ—¥æ±é›»å·¥", "7201.T": "æ—¥ç”£è‡ª",
    "7202.T": "ã„ã™ã‚", "7203.T": "ãƒˆãƒ¨ã‚¿", "7205.T": "æ—¥é‡è‡ª", "7211.T": "ä¸‰è±è‡ª",
    "7261.T": "ãƒãƒ„ãƒ€", "7267.T": "ãƒ›ãƒ³ãƒ€", "7269.T": "ã‚¹ã‚ºã‚­", "7270.T": "SUBARU",
    "7272.T": "ãƒ¤ãƒãƒç™º", "7453.T": "è‰¯å“è¨ˆç”»", "7731.T": "ãƒ‹ã‚³ãƒ³", "7733.T": "ã‚ªãƒªãƒ³ãƒ‘ã‚¹",
    "7735.T": "ã‚¹ã‚¯ãƒªãƒ³", "7741.T": "HOYA", "7751.T": "ã‚­ãƒ¤ãƒãƒ³", "7752.T": "ãƒªã‚³ãƒ¼",
    "7762.T": "ã‚·ãƒã‚ºãƒ³", "7832.T": "ãƒãƒ³ãƒŠãƒ HD", "7911.T": "å‡¸ç‰ˆå°åˆ·", "7912.T": "å¤§æ—¥æœ¬å°åˆ·",
    "7951.T": "ãƒ¤ãƒãƒ", "7974.T": "ä»»å¤©å ‚", "8001.T": "ä¼Šè—¤å¿ ", "8002.T": "ä¸¸ç´…",
    "8015.T": "è±Šç”°é€šå•†", "8031.T": "ä¸‰äº•ç‰©ç”£", "8035.T": "æ±ã‚¨ãƒ¬ã‚¯", "8053.T": "ä½å‹å•†äº‹",
    "8058.T": "ä¸‰è±å•†äº‹", "8113.T": "ãƒ¦ãƒ‹ãƒãƒ£ãƒ¼ãƒ ", "8252.T": "ä¸¸äº•G", "8253.T": "ã‚¯ãƒ¬ã‚»ã‚¾ãƒ³",
    "8267.T": "ã‚¤ã‚ªãƒ³", "8304.T": "ã‚ãŠãã‚‰éŠ€", "8306.T": "ä¸‰è±UFJ", "8308.T": "ã‚ŠããªHD",
    "8309.T": "ä¸‰äº•ä½å‹", "8316.T": "ä¸‰äº•ä½å‹ä¿¡è¨—", "8331.T": "åƒè‘‰éŠ€", "8354.T": "ãµããŠã‹FG",
    "8411.T": "ã¿ãšã»", "8591.T": "ã‚ªãƒªãƒƒã‚¯ã‚¹", "8601.T": "å¤§å’Œè¨¼G", "8604.T": "é‡æ‘HD",
    "8630.T": "ä½å‹ä¿¡è¨—", "8697.T": "æ—¥å–æ‰€", "8725.T": "MS&AD", "8750.T": "ç¬¬ä¸€ç”Ÿå‘½",
    "8766.T": "æ±äº¬æµ·ä¸Š", "8795.T": "T&DHD", "8801.T": "ä¸‰äº•ä¸", "8802.T": "ä¸‰è±åœ°æ‰€",
    "8804.T": "æ±äº¬å»ºç‰©", "8830.T": "ä½å‹ä¸", "9001.T": "æ±æ­¦", "9005.T": "æ±æ€¥",
    "9007.T": "å°ç”°æ€¥", "9008.T": "äº¬ç‹", "9009.T": "äº¬æˆ", "9020.T": "JRæ±æ—¥æœ¬",
    "9021.T": "JRè¥¿æ—¥æœ¬", "9022.T": "JRæ±æµ·", "9064.T": "ãƒ¤ãƒãƒˆHD", "9101.T": "æ—¥æœ¬éƒµèˆ¹",
    "9104.T": "å•†èˆ¹ä¸‰äº•", "9107.T": "å·å´æ±½èˆ¹", "9147.T": "NXHD", "9201.T": "JAL",
    "9202.T": "ANAHD", "9301.T": "ä¸‰è±å€‰åº«", "9432.T": "NTT", "9433.T": "KDDI",
    "9434.T": "ã‚½ãƒ•ãƒˆãƒãƒ³ã‚¯", "9501.T": "æ±é›»HD", "9502.T": "ä¸­éƒ¨é›»", "9503.T": "é–¢è¥¿é›»",
    "9531.T": "æ±ã‚¬ã‚¹", "9532.T": "å¤§é˜ªã‚¬ã‚¹", "9602.T": "æ±å®", "9613.T": "NTTãƒ‡ãƒ¼ã‚¿",
    "9735.T": "ã‚»ã‚³ãƒ ", "9766.T": "ã‚³ãƒŠãƒŸG", "9843.T": "ãƒ‹ãƒˆãƒªHD", "9983.T": "ãƒ•ã‚¡ãƒ¼ã‚¹ãƒˆãƒªãƒ†",
    "9984.T": "ã‚½ãƒ•ãƒˆãƒãƒ³ã‚¯G",
}

def fetch_market_data(tickers, lookback_days=DEFAULT_LOOKBACK_DAYS):
    end_dt = (now_jst().date() + timedelta(days=1)).isoformat()
    start_dt = (now_jst().date() - timedelta(days=lookback_days)).isoformat()
    raw = yf.download(
        tickers,
        start=start_dt,
        end=end_dt,
        interval="1d",
        auto_adjust=False,
        progress=False,
        group_by="column",  # (field, ticker)
        threads=True,
    )
    # å¿…é ˆã‚«ãƒ©ãƒ ãŒæƒã£ã¦ã„ã‚‹ã‹ç°¡æ˜“ãƒã‚§ãƒƒã‚¯
    for c in ("Close", "High", "Low"):
        if c not in raw.columns.get_level_values(0):
            raise RuntimeError(f"yfinance returned missing column: {c}")
    close = raw["Close"].copy()
    high = raw["High"].copy()
    low = raw["Low"].copy()
    return raw, close, high, low

# ===== æŠ¼ã—ç›®æŠ½å‡ºï¼ˆå³ã—ã„æ¡ä»¶ï¼‰ =====
def rolling_sma(series: pd.Series, window=SMA_WINDOW):
    return series.rolling(window, min_periods=window).mean()

def compute_one_ticker(close_s: pd.Series, high_s: pd.Series, low_s: pd.Series, window_days=30):
    """
    å˜ä¸€éŠ˜æŸ„ã®æŠ¼ã—ç›®å€™è£œã‚’1ä»¶è¿”ã™ã‹None
    å®šç¾©:
      - æœŸé–“å†…ã® High ã®ãƒ”ãƒ¼ã‚¯ã‚’ Peak ã¨ã™ã‚‹
      - Peak å½¢æˆå¾Œã®æœ€å®‰å€¤ã‚’ Pullback Low ã¨ã™ã‚‹
      - æœ€æ–°å€¤ Latest ã¯ series ã®æœ€å¾Œ
      - æŒ‡æ¨™:
         Rebound_From_Low_% = (Latest / PullbackLow - 1)*100
         Drop_From_Peak_%   = (1 - Latest / Peak) * 100
         Expected_Upper     = Peak
         Expected_Rise_%    = (Expected_Upper / Latest - 1)*100
         Days_Since_Pullback: Pullback Low ã‹ã‚‰æœ€æ–°ã¾ã§ã®å–¶æ¥­æ—¥æ•°
    æ¡ä»¶:
      Rebound>=REBOUND_MIN, Drop<=DROP_MAX, Days>=DAYS_SINCE_MIN,
      Latest >= SMA25, Expected_Rise_% >= EXPECTED_RISE_MIN,
      Latest >= Pullback Low
    """
    try:
        close_s = close_s.dropna()
        high_s = high_s.reindex_like(close_s).dropna()
        low_s  = low_s.reindex_like(close_s).dropna()
        if len(close_s) < max(SMA_WINDOW, window_days) + 2:
            return None

        # å¯¾è±¡æœŸé–“
        look = close_s.iloc[-window_days:]
        look_high = high_s.loc[look.index]
        look_low  = low_s.loc[look.index]

        if look_high.empty or look_low.empty:
            return None

        # ãƒ”ãƒ¼ã‚¯ï¼ˆæœŸé–“å†…ã®æœ€é«˜å€¤ï¼‰
        peak_idx = look_high.idxmax()
        peak_val = float(look_high.loc[peak_idx])

        # ãƒ”ãƒ¼ã‚¯å¾Œã®æœ€å®‰å€¤ï¼ˆãªã‘ã‚Œã°é™¤å¤–ï¼‰
        after_peak = look_low.loc[look_low.index > peak_idx]
        if after_peak.empty:
            return None
        pull_idx = after_peak.idxmin()
        pull_val = float(after_peak.loc[pull_idx])

        latest_idx = close_s.index[-1]
        latest_val = float(close_s.iloc[-1])
        prev_val = float(close_s.iloc[-2]) if len(close_s) >= 2 else np.nan

        sma25 = float(rolling_sma(close_s).iloc[-1]) if len(close_s) >= SMA_WINDOW else np.nan

        rebound_pct = (latest_val / pull_val - 1.0) * 100.0
        drop_pct = (1.0 - latest_val / peak_val) * 100.0
        expected_upper = peak_val
        expected_rise_pct = (expected_upper / latest_val - 1.0) * 100.0
        days_since_pull = (close_s.index.get_loc(latest_idx) - close_s.index.get_loc(pull_idx))


        # æ¡ä»¶åˆ¤å®š
        conds = [
            rebound_pct >= REBOUND_MIN,
            rebound_pct <= REBOUND_MAX,
            drop_pct <= DROP_MAX,
            days_since_pull >= DAYS_SINCE_MIN,
            not math.isnan(sma25) and latest_val >= sma25,
            expected_rise_pct >= EXPECTED_RISE_MIN,
            latest_val >= pull_val,
        ]
        if not all(conds):
            return None

        return {
            "Ticker": close_s.name,
            "Peak_Date": peak_idx.date(),
            "Peak_High": round(peak_val, 2),
            "Pullback_Date": pull_idx.date(),
            "Pullback_Low": round(pull_val, 2),
            "Latest_Date": latest_idx.date(),
            "Latest_Close": round(latest_val, 2),
            "Prev_Close": round(prev_val, 2) if not math.isnan(prev_val) else np.nan,
            "Return_%": round(expected_rise_pct, 2),
            "Rebound_From_Low_%": round(rebound_pct, 2),
            "Drop_From_Peak_%": round(drop_pct, 2),
            "Days_Since_Pullback": int(days_since_pull),
            "SMA25": round(sma25, 2) if not math.isnan(sma25) else np.nan,
            "Expected_Upper": round(expected_upper, 2),
            "Expected_Rise_%": round(expected_rise_pct, 2),
        }
    except Exception as e:
        # å€‹åˆ¥éŠ˜æŸ„ã§ã®è¨ˆç®—å¤±æ•—ã¯ã‚¹ã‚­ãƒƒãƒ—
        print(f"[WARN] compute_one_ticker failed for {close_s.name}: {e}", file=sys.stderr)
        return None

def find_pullback_candidates(close_df: pd.DataFrame, high_df: pd.DataFrame, low_df: pd.DataFrame, window_days=30):
    rows = []
    for ticker in close_df.columns:
        res = compute_one_ticker(close_df[ticker], high_df[ticker], low_df[ticker], window_days=window_days)
        if res:
            rows.append(res)
    if not rows:
        return pd.DataFrame()
    df = pd.DataFrame(rows)
    # ä¸¦ã¹æ›¿ãˆï¼ˆæœŸå¾…ä¸Šæ˜‡ç‡ã®é™é †ï¼‰
    df = df.sort_values("Return_%", ascending=False).reset_index(drop=True)
    return df

# ===== ãƒãƒ£ãƒ¼ãƒˆç”»åƒä½œæˆ =====
def save_chart_image_from_raw(raw_df, ticker: str, out_dir="charts"):
    """
    raw_df: yf.download(..., group_by='column') ã® MultiIndex DataFrame
    """
    need_cols = ["Open", "High", "Low", "Close", "Volume"]
    try:
        use = raw_df.loc[:, [(c, ticker) for c in need_cols]].copy()
    except Exception:
        return None
    if use.empty:
        return None
    use.columns = need_cols
    use = use.dropna()
    if use.empty:
        return None

    os.makedirs(out_dir, exist_ok=True)
    out_path = os.path.join(out_dir, f"{ticker}.png")
    mpf.plot(
        use,
        type="candle",
        mav=(5, 25, 75),
        volume=True,
        style="yahoo",
        savefig=dict(fname=out_path, dpi=140, bbox_inches="tight")
    )
    return out_path

# ===== åç§°è¾æ›¸ï¼ˆå¿…è¦ãªã‚‰æ‹¡å¼µ/å·®ã—æ›¿ãˆï¼‰=====
def build_ticker_name_map(tickers):
    # æœ€ä½é™: TSEã¯ yfinance ä¸Šã€Œ.Tã€ã‚µãƒ•ã‚£ãƒƒã‚¯ã‚¹ã€éŠ˜æŸ„åã¯APIã§é€æ¬¡å–å¾—ã™ã‚‹ã¨é‡ã„ã®ã§ç©ºã«ã—ã¦ãŠã
    # å¿…è¦ã«å¿œã˜ã¦CSVã‹ã‚‰èª­ã¿è¾¼ã‚€ãªã©ã«å·®ã—æ›¿ãˆ
    return {t: "" for t in tickers}

# ===== ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ =====
def run_pipeline():
    tickers = load_tickers()
    raw, close, high, low = fetch_market_data(tickers, lookback_days=DEFAULT_LOOKBACK_DAYS)

    # 30æ—¥ãƒ»14æ—¥ã§æŠ½å‡º â†’ ãƒãƒ¼ã‚¸ï¼ˆåŒä¸€ãƒ†ã‚£ãƒƒã‚«ãƒ¼ã¯ 'Return_%' ãŒå¤§ãã„æ–¹ã‚’æ¡ç”¨ï¼‰
    rs = []
    for w in (30, 14):
        df = find_pullback_candidates(close, high, low, window_days=w)
        if not df.empty:
            df["Window"] = w
            rs.append(df)

    if not rs:
        return pd.DataFrame(), raw, {}

    cat = pd.concat(rs, ignore_index=True).sort_values(["Ticker", "Return_%"], ascending=[True, False])
    best = cat.groupby("Ticker", as_index=False).first().sort_values("Return_%", ascending=False).reset_index(drop=True)
    ticker_name_map = build_ticker_name_map(best["Ticker"].tolist())
    return best, raw, ticker_name_map

# ===== é€šçŸ¥ =====
def notify(best_df: pd.DataFrame, raw_df, ticker_name_map: dict, top_n=TOP_N):
    if best_df is None or best_df.empty:
        line_push_text("ã€æŠ¼ã—ç›®ã‚¹ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã€‘æœ¬æ—¥ã¯æŠ½å‡ºãªã—ã§ã—ãŸã€‚")
        return

    header = (
        f"ğŸ“Šã€æŠ¼ã—ç›®ã‚¹ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã€‘{now_jst().strftime('%m/%d %H:%M')}\n"
        f"æŠ½å‡º: {len(best_df)} éŠ˜æŸ„ï¼ˆé‡è¤‡çµ±åˆï¼‰\n"
        f"æ¡ä»¶: åç™ºâ‰¥{REBOUND_MIN:.0f}%ãƒ»ä¸‹è½â‰¤{DROP_MAX:.0f}%ãƒ»SMA25ä¸Šãƒ»æœŸå¾…â‰¥{EXPECTED_RISE_MIN:.0f}%ãƒ»{DAYS_SINCE_MIN}æ—¥çµŒé\n"
        f"------------------------------\n"
    )
    send_long_text(header)

    # éŠ˜æŸ„ã”ã¨ã«ã€Œ4è¡Œãƒ†ã‚­ã‚¹ãƒˆâ†’ç”»åƒã€ã‚’é€ã‚‹
    for _, r in best_df.head(top_n).iterrows():
        ticker = str(r["Ticker"])
        name = ticker_name_map.get(ticker, "")
        upper  = r.get("Expected_Upper")
        latest = r.get("Latest_Close")
        low    = r.get("Pullback_Low")
        rise_p = r.get("Expected_Rise_%")
        prev   = r.get("Prev_Close")

        def fnum(x):
            try: return f"{float(x):,.0f}"
            except: return "-"
        def fpct(x):
            try: return f"{float(x):.1f}%"
            except: return "-"
        def fpct_signed(x):
            try:
                x = float(x)
                if not np.isfinite(x): return "-"
                return f"{x:+.1f}%"
            except:
                return "-"

        expect_amt = (float(upper) - float(latest)) if pd.notna(upper) and pd.notna(latest) else None
        chg_pct = ((float(latest) / float(prev)) - 1.0) * 100.0 if (pd.notna(latest) and pd.notna(prev) and float(prev) != 0.0) else None
        bot_pct = ((float(latest) / float(low)) - 1.0) * 100.0 if (pd.notna(latest) and pd.notna(low) and float(low) != 0.0) else None

        line1 = f"{ticker} {name}".rstrip()
        line2 = f"â†— {fpct(rise_p)}   ğŸ¯ ä¸Š {fnum(upper)}   ä¸‹ {fnum(low)}"
        line3 = f"ä»Š {fnum(latest)}   ğŸ¯ æœŸå¾…é¡ {fnum(expect_amt)}"
        line4 = f"å¤‰å‹•ç‡ {fpct_signed(chg_pct)}   åº•å€¤æ¯”è¼ƒ {fpct_signed(bot_pct)}"

        # â‘  4è¡Œãƒ†ã‚­ã‚¹ãƒˆ
        send_long_text("\n".join([line1, line2, line3, line4]))

        # â‘¡ ãƒãƒ£ãƒ¼ãƒˆç”»åƒï¼ˆåŒã˜ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ä½œå›³ï¼‰â†’ å…¬é–‹URLã§é€ä¿¡
        img_path = save_chart_image_from_raw(raw_df, ticker, out_dir="charts")
        if img_path and PUBLIC_BASE_URL:
            public_url = f"{PUBLIC_BASE_URL}/{os.path.basename(img_path)}"
            line_push_image(public_url)

def main():
    now = now_jst()
    force = os.getenv("FORCE_RUN") == "1"

    if not force and is_weekend(now):
        print(f"[SKIP] {now:%F %R} é€±æœ«ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—ï¼ˆFORCE_RUN=1ã§å®Ÿè¡Œå¯èƒ½ï¼‰")
        return

    best, raw, name_map = run_pipeline()
    notify(best, raw, name_map, top_n=TOP_N)

if __name__ == "__main__":
    main()
