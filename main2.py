
import os
import sys
import json
import math
from datetime import datetime, timedelta
import requests
import numpy as np
import pandas as pd
import yfinance as yf
import mplfinance as mpf

# ===== è¨­å®šï¼ˆå¿…è¦ã«å¿œã˜ã¦å¤‰æ›´ï¼‰ =====
DEPTH_MIN_PCT      = float(os.getenv("DEPTH_MIN_PCT", "0.06"))  # Peakâ†’Low ã®ä¸‹è½ç‡ ä¸‹é™ï¼ˆä¾‹ 6%ï¼‰
ATR20_MULT_MIN     = float(os.getenv("ATR20_MULT_MIN", "1.0"))  # Peakâ†’Low ã®ä¸‹è½å¹…ãŒ ATR20 ã®ä½•å€ä»¥ä¸Šã‹
SMA_TOUCH_TOL      = float(os.getenv("SMA_TOUCH_TOL", "0.02"))  # SMA25 ã¸ã®ã‚¿ãƒƒãƒè¨±å®¹ Â±2%
DAYS_SINCE_LOW_MIN = int(os.getenv("DAYS_SINCE_LOW_MIN", "3"))  # Low ã‹ã‚‰ã®çµŒéæœ€å°æ—¥æ•°
TZ_OFFSET = 9  # JST
REBOUND_MIN = 0.01       # åç™ºç‡ >= 1%
REBOUND_MAX = 0.04       # åç™ºç‡ <= 4%
DROP_MAX = 15.0         # ãƒ”ãƒ¼ã‚¯ã‹ã‚‰ã®è¨±å®¹ä¸‹è½ç‡ <= 15%
DAYS_SINCE_MIN = 2      # æŠ¼ã—ç›®ã‹ã‚‰æœ€æ–°ã¾ã§ã®å–¶æ¥­æ—¥æ•° >= 2
EXPECTED_RISE_MIN = 3.0 # æœŸå¾…ä¸Šæ˜‡ç‡ >= 3%
SMA_WINDOW = 25
TOP_N = 15              # é€ä¿¡ä¸Šé™
DEFAULT_LOOKBACK_DAYS = int(os.getenv("LOOKBACK_DAYS", "180"))

# ===== Discord Webhook =====
DISCORD_WEBHOOK_URL = os.environ.get("DISCORD_WEBHOOK_URL", "").strip()
PUBLIC_BASE_URL = os.environ.get("PUBLIC_BASE_URL", "").rstrip("/")

if not DISCORD_WEBHOOK_URL:
    print("[ERROR] Set DISCORD_WEBHOOK_URL env.", file=sys.stderr)

# ===== ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ =====
def now_jst():
    return datetime.utcnow() + timedelta(hours=TZ_OFFSET)

def is_weekend(dt: datetime) -> bool:
    # åœŸæ—¥ã‚¹ã‚­ãƒƒãƒ—ï¼ˆæ—¥æœ¬ã®ç¥æ—¥ã¯è€ƒæ…®ã—ãªã„ï¼‰
    return dt.weekday() >= 5

def chunk_text(text: str, limit: int = 1900):
    """
    Discordã®contentåˆ¶é™(2000æ–‡å­—)ã‚’è€ƒæ…®ã—ã¦åˆ†å‰²ï¼ˆä½™è£•1900ï¼‰ã€‚
    æ”¹è¡Œå˜ä½ã§åˆ†å‰²ã€‚é•·ã™ãã‚‹1è¡Œã¯å¼·åˆ¶çš„ã«åˆ‡ã‚‹ã€‚
    """
    out, buf, size = [], [], 0
    for line in text.splitlines():
        if len(line) > limit:
            # 1è¡ŒãŒè¶…é•·ã„ã¨ãã¯å¼·åˆ¶åˆ†å‰²
            while len(line) > limit:
                part = line[:limit]
                line = line[limit:]
                if buf:
                    out.append("\n".join(buf))
                    buf, size = [], 0
                out.append(part)
            if line == "":
                continue
        add = len(line) + 1
        if size + add > limit:
            out.append("\n".join(buf))
            buf, size = [line], len(line) + 1
        else:
            buf.append(line)
            size += add
    if buf:
        out.append("\n".join(buf))
    return out

# ===== æ—¥çµŒ225ãƒ†ã‚£ãƒƒã‚«ãƒ¼ =====
nikkei225_tickers = [ '4151.T','4502.T','4503.T','4506.T','4507.T','4519.T','4523.T','4568.T','4578.T','6479.T','6501.T','6503.T','6504.T','6506.T','6526.T','6594.T','6645.T','6674.T','6701.T','6702.T','6723.T','6724.T','6752.T','6753.T','6758.T','6762.T','6770.T','6841.T','6857.T','6861.T','6902.T','6920.T','6952.T','6954.T','6971.T','6976.T','6981.T','7735.T','7751.T','7752.T','8035.T','7201.T','7202.T','7203.T','7205.T','7211.T','7261.T','7267.T','7269.T','7270.T','7272.T','4543.T','4902.T','6146.T','7731.T','7733.T','7741.T','7762.T','9432.T','9433.T','9434.T','9613.T','9984.T','5831.T','7186.T','8304.T','8306.T','8308.T','8309.T','8316.T','8331.T','8354.T','8411.T','8253.T','8591.T','8697.T','8601.T','8604.T','8630.T','8725.T','8750.T','8766.T','8795.T','1332.T','2002.T','2269.T','2282.T','2501.T','2502.T','2503.T','2801.T','2802.T','2871.T','2914.T','3086.T','3092.T','3099.T','3382.T','7453.T','8233.T','8252.T','8267.T','9843.T','9983.T','2413.T','2432.T','3659.T','4307.T','4324.T','4385.T','4661.T','4689.T','4704.T','4751.T','4755.T','6098.T','6178.T','7974.T','9602.T','9735.T','9766.T','1605.T','3401.T','3402.T','3861.T','3405.T','3407.T','4004.T','4005.T','4021.T','4042.T','4043.T','4061.T','4063.T','4183.T','4188.T','4208.T','4452.T','4901.T','4911.T','6988.T','5019.T','5020.T','5101.T','5108.T','5201.T','5214.T','5233.T','5301.T','5332.T','5333.T','5401.T','5406.T','5411.T','3436.T','5706.T','5711.T','5713.T','5714.T','5801.T','5802.T','5803.T','2768.T','8001.T','8002.T','8015.T','8031.T','8053.T','8058.T','1721.T','1801.T','1802.T','1803.T','1808.T','1812.T','1925.T','1928.T','1963.T','5631.T','6103.T','6113.T','6273.T','6301.T','6302.T','6305.T','6326.T','6361.T','6367.T','6471.T','6472.T','6473.T','7004.T','7011.T','7013.T','7012.T','7832.T','7911.T','7912.T','7951.T','3289.T','8801.T','8802.T','8804.T','8830.T','9001.T','9005.T','9007.T','9008.T','9009.T','9020.T','9021.T','9022.T','9064.T','9147.T','9101.T','9104.T','9107.T','9201.T','9202.T','9301.T','9501.T','9502.T','9503.T','9531.T','9532.T' ]

# ===== çŸ­ç¸®åãƒãƒƒãƒ— =====
ticker_name_map = {
    "1332.T": "æ—¥æ°´", "1333.T": "ãƒãƒ«ãƒãƒ‹ãƒãƒ­", "1605.T": "INPEX", "1801.T": "å¤§æˆå»º",
    "1802.T": "æ¸…æ°´å»º", "1803.T": "é£›å³¶å»º", "1808.T": "é•·è°·å·¥", "1812.T": "é¹¿å³¶",
    "1925.T": "å¤§å’Œãƒã‚¦ã‚¹", "1928.T": "ç©æ°´ãƒã‚¦ã‚¹", "1963.T": "æ—¥æ®HD", "2002.T": "æ—¥æ¸…ç²‰G",
    "2269.T": "æ˜æ²»HD", "2282.T": "æ—¥æœ¬ãƒãƒ ", "2413.T": "ã‚¨ãƒ ã‚¹ãƒªãƒ¼", "2432.T": "DeNA",
    "2501.T": "ã‚µãƒƒãƒãƒ­HD", "2502.T": "ã‚¢ã‚µãƒ’GHD", "2503.T": "ã‚­ãƒªãƒ³HD", "2768.T": "åŒæ—¥",
    "2801.T": "ã‚­ãƒƒã‚³ãƒãƒ³", "2802.T": "å‘³ã®ç´ ", "2871.T": "ãƒ‹ãƒãƒ¬ã‚¤", "2914.T": "JT",
    "3086.T": "Jãƒ•ãƒ­ãƒ³ãƒˆ", "3092.T": "ZOZO", "3099.T": "ä¸‰è¶Šä¼Šå‹¢ä¸¹", "3382.T": "ã‚»ãƒ–ãƒ³&ã‚¢ã‚¤",
    "3401.T": "å¸äºº", "3402.T": "æ±ãƒ¬", "3405.T": "ã‚¯ãƒ©ãƒ¬", "3407.T": "æ—­åŒ–æˆ",
    "3436.T": "SUMCO", "3861.T": "ç‹å­HD", "4004.T": "æ˜­é›»å·¥", "4005.T": "ä½å‹åŒ–å­¦",
    "4021.T": "æ—¥ç”£åŒ–", "4042.T": "æ±ã‚½ãƒ¼", "4043.T": "ãƒˆã‚¯ãƒ¤ãƒ", "4061.T": "é›»åŒ–",
    "4063.T": "ä¿¡è¶ŠåŒ–", "4183.T": "ä¸‰äº•åŒ–å­¦", "4188.T": "ä¸‰è±ã‚±ãƒŸHD", "4208.T": "UBE",
    "4452.T": "èŠ±ç‹", "4502.T": "æ­¦ç”°è–¬å“", "4503.T": "ã‚¢ã‚¹ãƒ†ãƒ©ã‚¹", "4506.T": "å¤§æ—¥æœ¬ä½å‹",
    "4507.T": "å¡©é‡ç¾©", "4519.T": "ä¸­å¤–è£½è–¬", "4523.T": "ã‚¨ãƒ¼ã‚¶ã‚¤", "4543.T": "ãƒ†ãƒ«ãƒ¢",
    "4568.T": "ç¬¬ä¸€ä¸‰å…±", "4578.T": "å¤§å¡šHD", "4661.T": "OLC", "4689.T": "ZHD",
    "4704.T": "ãƒˆãƒ¬ãƒ³ãƒ‰", "4751.T": "ã‚µã‚¤ãƒãƒ¼", "4755.T": "æ¥½å¤©G", "4901.T": "å¯Œå£«ãƒ•ã‚¤ãƒ«ãƒ ",
    "4902.T": "ã‚³ãƒ‹ã‚«ãƒŸãƒãƒ«ã‚¿", "4911.T": "è³‡ç”Ÿå ‚", "5020.T": "ENEOS",
    "5101.T": "æ¨ªæµœã‚´ãƒ ", "5108.T": "ãƒ–ãƒªãƒ‚ã‚¹ãƒˆãƒ³", "5201.T": "AGC", "5214.T": "æ—¥é›»ç¡",
    "5233.T": "å¤ªå¹³æ´‹ã‚»ãƒ¡", "5301.T": "æ±æµ·ã‚«ãƒ¼ãƒœãƒ³", "5332.T": "TOTO", "5333.T": "æ—¥æœ¬ã‚¬ã‚¤ã‚·",
    "5401.T": "æ—¥æœ¬è£½é‰„", "5406.T": "ç¥æˆ¸è£½é‹¼", "5411.T": "JFEHD", "5706.T": "ä¸‰äº•é‡‘å±",
    "5711.T": "ä¸‰è±ãƒãƒ†", "5713.T": "ä½å‹é‡‘å±é‰±å±±", "5714.T": "DOWA", "5801.T": "å¤æ²³é›»å·¥",
    "5802.T": "ä½å‹é›»å·¥", "5803.T": "ãƒ•ã‚¸ã‚¯ãƒ©", "6098.T": "ãƒªã‚¯ãƒ«ãƒ¼ãƒˆHD", "6178.T": "æ—¥æœ¬éƒµæ”¿",
    "6273.T": "SMC", "6301.T": "ã‚³ãƒãƒ„", "6302.T": "ä½å‹é‡æ©Ÿ", "6305.T": "æ—¥ç«‹å»ºæ©Ÿ",
    "6326.T": "ã‚¯ãƒœã‚¿", "6361.T": "èåŸ", "6367.T": "ãƒ€ã‚¤ã‚­ãƒ³", "6471.T": "æ—¥ç²¾å·¥",
    "6472.T": "NTN", "6473.T": "ã‚¸ã‚§ã‚¤ãƒ†ã‚¯ãƒˆ", "6479.T": "ãƒŸãƒãƒ™ã‚¢ãƒŸãƒ„ãƒŸ", "6501.T": "æ—¥ç«‹",
    "6503.T": "ä¸‰è±é›»æ©Ÿ", "6504.T": "å¯Œå£«é›»æ©Ÿ", "6506.T": "å®‰å·é›»æ©Ÿ", "6526.T": "ã‚½ã‚·ã‚ªãƒã‚¯ã‚¹ãƒˆ",
    "6594.T": "æ—¥é›»ç”£", "6645.T": "ã‚ªãƒ ãƒ­ãƒ³", "6674.T": "ã‚¸ãƒ¼ã‚¨ã‚¹ãƒ¦ã‚¢ã‚µ", "6701.T": "NEC",
    "6702.T": "å¯Œå£«é€š", "6723.T": "ãƒ«ãƒã‚µã‚¹", "6724.T": "ã‚»ã‚¤ã‚³ãƒ¼ã‚¨ãƒ—ã‚½ãƒ³", "6752.T": "ãƒ‘ãƒŠã‚½ãƒ‹ãƒƒã‚¯",
    "6753.T": "ã‚·ãƒ£ãƒ¼ãƒ—", "6758.T": "ã‚½ãƒ‹ãƒ¼G", "6762.T": "TDK", "6770.T": "ã‚¢ãƒ«ãƒ—ã‚¹ã‚¢ãƒ«ãƒ‘",
    "6841.T": "æ¨ªæ²³é›»æ©Ÿ", "6857.T": "ã‚¢ãƒ‰ãƒ†ã‚¹ãƒˆ", "6861.T": "ã‚­ãƒ¼ã‚¨ãƒ³ã‚¹", "6902.T": "ãƒ‡ãƒ³ã‚½ãƒ¼",
    "6920.T": "ãƒ¬ãƒ¼ã‚¶ãƒ¼ãƒ†ãƒƒã‚¯", "6952.T": "ã‚«ã‚·ã‚ª", "6954.T": "ãƒ•ã‚¡ãƒŠãƒƒã‚¯", "6971.T": "äº¬ã‚»ãƒ©",
    "6976.T": "å¤ªé™½èª˜é›»", "6981.T": "æ‘ç”°è£½ä½œæ‰€", "6988.T": "æ—¥æ±é›»å·¥", "7201.T": "æ—¥ç”£è‡ª",
    "7202.T": "ã„ã™ã‚", "7203.T": "ãƒˆãƒ¨ã‚¿", "7205.T": "æ—¥é‡è‡ª", "7211.T": "ä¸‰è±è‡ª",
    "7261.T": "ãƒãƒ„ãƒ€", "7267.T": "ãƒ›ãƒ³ãƒ€", "7269.T": "ã‚¹ã‚ºã‚­", "7270.T": "SUBARU",
    "7272.T": "ãƒ¤ãƒãƒç™º", "7453.T": "è‰¯å“è¨ˆç”»", "7731.T": "ãƒ‹ã‚³ãƒ³", "7733.T": "ã‚ªãƒªãƒ³ãƒ‘ã‚¹",
    "7735.T": "ã‚¹ã‚¯ãƒªãƒ³", "7741.T": "HOYA", "7751.T": "ã‚­ãƒ¤ãƒãƒ³", "7752.T": "ãƒªã‚³ãƒ¼",
    "7762.T": "ã‚·ãƒã‚ºãƒ³", "7832.T": "ãƒãƒ³ãƒŠãƒ HD", "7911.T": "å‡¸ç‰ˆå°åˆ·", "7912.T": "å¤§æ—¥æœ¬å°åˆ·",
    "7951.T": "ãƒ¤ãƒãƒ", "7974.T": "ä»»å¤©å ‚", "8001.T": "ä¼Šè—¤å¿ ", "8002.T": "ä¸¸ç´…",
    "8015.T": "è±Šç”°é€šå•†", "8031.T": "ä¸‰äº•ç‰©ç”£", "8035.T": "æ±ã‚¨ãƒ¬ã‚¯", "8053.T": "ä½å‹å•†äº‹",
    "8058.T": "ä¸‰è±å•†äº‹", "8113.T": "ãƒ¦ãƒ‹ãƒãƒ£ãƒ¼ãƒ ", "8252.T": "ä¸¸äº•G", "8253.T": "ã‚¯ãƒ¬ã‚»ã‚¾ãƒ³",
    "8267.T": "ã‚¤ã‚ªãƒ³", "8304.T": "ã‚ãŠãã‚‰éŠ€", "8306.T": "ä¸‰è±UFJ", "8308.T": "ã‚ŠããªHD",
    "8309.T": "ä¸‰äº•ä½å‹", "8316.T": "ä¸‰äº•ä½å‹ä¿¡è¨—", "8331.T": "åƒè‘‰éŠ€", "8354.T": "ãµããŠã‹FG",
    "8411.T": "ã¿ãšã»", "8591.T": "ã‚ªãƒªãƒƒã‚¯ã‚¹", "8601.T": "å¤§å’Œè¨¼G", "8604.T": "é‡æ‘HD",
    "8630.T": "ä½å‹ä¿¡è¨—", "8697.T": "æ—¥å–æ‰€", "8725.T": "MS&AD", "8750.T": "ç¬¬ä¸€ç”Ÿå‘½",
    "8766.T": "æ±äº¬æµ·ä¸Š", "8795.T": "T&DHD", "8801.T": "ä¸‰äº•ä¸", "8802.T": "ä¸‰è±åœ°æ‰€",
    "8804.T": "æ±äº¬å»ºç‰©", "8830.T": "ä½å‹ä¸", "9001.T": "æ±æ­¦", "9005.T": "æ±æ€¥",
    "9007.T": "å°ç”°æ€¥", "9008.T": "äº¬ç‹", "9009.T": "äº¬æˆ", "9020.T": "JRæ±æ—¥æœ¬",
    "9021.T": "JRè¥¿æ—¥æœ¬", "9022.T": "JRæ±æµ·", "9064.T": "ãƒ¤ãƒãƒˆHD", "9101.T": "æ—¥æœ¬éƒµèˆ¹",
    "9104.T": "å•†èˆ¹ä¸‰äº•", "9107.T": "å·å´æ±½èˆ¹", "9147.T": "NXHD", "9201.T": "JAL",
    "9202.T": "ANAHD", "9301.T": "ä¸‰è±å€‰åº«", "9432.T": "NTT", "9433.T": "KDDI",
    "9434.T": "ã‚½ãƒ•ãƒˆãƒãƒ³ã‚¯", "9501.T": "æ±é›»HD", "9502.T": "ä¸­éƒ¨é›»", "9503.T": "é–¢è¥¿é›»",
    "9531.T": "æ±ã‚¬ã‚¹", "9532.T": "å¤§é˜ªã‚¬ã‚¹", "9602.T": "æ±å®", "9613.T": "NTTãƒ‡ãƒ¼ã‚¿",
    "9735.T": "ã‚»ã‚³ãƒ ", "9766.T": "ã‚³ãƒŠãƒŸG", "9843.T": "ãƒ‹ãƒˆãƒªHD", "9983.T": "ãƒ•ã‚¡ãƒ¼ã‚¹ãƒˆãƒªãƒ†",
    "9984.T": "ã‚½ãƒ•ãƒˆãƒãƒ³ã‚¯G",
}

# ======= RSI è¨ˆç®—ãƒ˜ãƒ«ãƒ‘ãƒ¼ï¼ˆLINEç‰ˆã‹ã‚‰è¸è¥²ï¼‰ =======
def latest_rsi_from_raw(raw_df, ticker: str, period: int = 14):
    """
    yf.download(..., group_by='column') ã®ç”Ÿãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å¯¾è±¡ãƒ†ã‚£ãƒƒã‚«ãƒ¼ã®çµ‚å€¤ã§RSI(14)ã‚’ç®—å‡ºã€‚
    å–å¾—ä¸å¯ã®å ´åˆã¯ None ã‚’è¿”ã™ã€‚
    """
    try:
        if isinstance(raw_df.columns, pd.MultiIndex):
            close = raw_df[("Close", ticker)].dropna()
        else:
            close = raw_df["Close"].dropna()
        if len(close) < period + 2:
            return None
        delta = close.diff()
        up = delta.clip(lower=0.0)
        down = (-delta).clip(lower=0.0)
        roll_up = up.rolling(period, min_periods=period).mean()
        roll_down = down.rolling(period, min_periods=period).mean()
        rs = roll_up / roll_down.replace(0, np.nan)
        rsi = 100.0 - (100.0 / (1.0 + rs))
        return float(rsi.iloc[-1])
    except Exception:
        return None

# ===== Discordé€ä¿¡ =====
def discord_send_content(msg: str):
    r = requests.post(
        DISCORD_WEBHOOK_URL,
        json={"content": msg},
        headers={"Content-Type": "application/json"},
        timeout=30,
    )
    if r.status_code >= 300:
        raise RuntimeError(f"Discord content failed: {r.status_code} {r.text}")


def discord_send_embed(
    title: str,
    description: str | None = None,
    image_url: str | None = None,
    fields: list | None = None,
):
    embed = {"title": title, "timestamp": datetime.utcnow().isoformat() + "Z"}
    if description:
        embed["description"] = description
    if image_url:
        embed["image"] = {"url": image_url}
    if fields:
        embed["fields"] = fields

    r = requests.post(
        DISCORD_WEBHOOK_URL,
        json={"embeds": [embed]},
        headers={"Content-Type": "application/json"},
        timeout=30,
    )
    if r.status_code >= 300:
        raise RuntimeError(f"Discord embed failed: {r.status_code} {r.text}")


def discord_send_image_file(
    file_path: str,
    title: str,
    description: str | None = None,
    fields: list | None = None,
):
    """ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’Webhookã«ç›´æ¥æ·»ä»˜ã—ã¦é€ã‚‹ï¼ˆå¤–éƒ¨URLä¸è¦ï¼‰"""
    embed = {"title": title, "timestamp": datetime.utcnow().isoformat() + "Z"}
    if description:
        embed["description"] = description
    if fields:
        embed["fields"] = fields

    filename = os.path.basename(file_path)
    # æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«ã¯ attachment://<filename> ã§å‚ç…§
    embed["image"] = {"url": f"attachment://{filename}"}

    with open(file_path, "rb") as f:
        files = {"file": (filename, f, "image/png")}
        data = {"payload_json": json.dumps({"embeds": [embed]})}
        r = requests.post(DISCORD_WEBHOOK_URL, files=files, data=data, timeout=30)
        if r.status_code >= 300:
            raise RuntimeError(
                f"Discord image upload failed: {r.status_code} {r.text}"
            )


def send_long_text(msg: str):
    discord_send_content(msg)

# ===== ãƒ‡ãƒ¼ã‚¿å–å¾— =====
def load_tickers():
    # å„ªå…ˆ: ç’°å¢ƒå¤‰æ•° TICKERS_CSV ã®CSVï¼ˆTicker/Symbol/Codeåˆ—ï¼‰
    csv_path = os.getenv("TICKERS_CSV")
    if csv_path and os.path.exists(csv_path):
        df = pd.read_csv(csv_path)
        col = None
        for c in df.columns:
            if c.lower() in ("ticker", "symbol", "code"):
                col = c
                break
        if col:
            tickers = [str(x).strip() for x in df[col].dropna().unique().tolist()]
            if tickers:
                return tickers

    # ã‚µãƒ³ãƒ—ãƒ«ï¼ˆã‚ã¨ã§å…¨éŠ˜æŸ„ã«å·®ã—æ›¿ãˆOKï¼‰
    return nikkei225_tickers

def fetch_market_data(tickers, lookback_days=DEFAULT_LOOKBACK_DAYS):
    end_dt = (now_jst().date() + timedelta(days=1)).isoformat()
    start_dt = (now_jst().date() - timedelta(days=lookback_days)).isoformat()
    raw = yf.download(
        tickers,
        start=start_dt,
        end=end_dt,
        interval="1d",
        auto_adjust=False,
        progress=False,
        group_by="column",  # (field, ticker)
        threads=True,
    )
    # å¿…é ˆã‚«ãƒ©ãƒ ãŒæƒã£ã¦ã„ã‚‹ã‹ç°¡æ˜“ãƒã‚§ãƒƒã‚¯
    for c in ("Close", "High", "Low"):
        if c not in raw.columns.get_level_values(0):
            raise RuntimeError(f"yfinance returned missing column: {c}")
    close = raw["Close"].copy()
    high = raw["High"].copy()
    low = raw["Low"].copy()
    return raw, close, high, low

# ===== æŠ¼ã—ç›®æŠ½å‡ºï¼ˆå³ã—ã„æ¡ä»¶ãƒ»LINEç‰ˆè¸è¥²ï¼‰ =====

def _ema(s: pd.Series, span: int) -> pd.Series:
    return s.ewm(span=span, adjust=False, min_periods=span).mean()

def _atr20(high: pd.Series, low: pd.Series, close: pd.Series) -> pd.Series:
    """ATR(20) = 20æ—¥EMA(TR)ã€‚TR=max(H-L, |H-PrevC|, |L-PrevC|)"""
    prev_close = close.shift(1)
    tr = pd.concat([
        (high - low).abs(),
        (high - prev_close).abs(),
        (low  - prev_close).abs()
    ], axis=1).max(axis=1)
    return _ema(tr, 20)

def _zigzag_last_swing_low(high: pd.Series, low: pd.Series, close: pd.Series,
                           thresh_pct: float, atr_mult: float) -> tuple[pd.Timestamp, float] | None:
    """
    ç›´è¿‘windowå†…ã§â€œæ„å‘³ã®ã‚ã‚‹è°·ï¼ˆSwing Lowï¼‰â€ã‚’1ã¤è¿”ã™ï¼ˆZigZagé¢¨ï¼‰
    ãƒ»ç›´è¿‘ã®å±€æ‰€é«˜å€¤ã‹ã‚‰é–¾å€¤ï¼ˆ% ã¾ãŸã¯ ATR20Ã—multï¼‰ä»¥ä¸Šä¸‹è½â†’è°·å€™è£œç¢ºå®š
    ãƒ»ãã®å¾Œã®ä¸Šæ˜‡ã§ãƒ”ãƒœãƒƒãƒˆç¢ºå®š
    æˆ»ã‚Šå€¤: (å®‰å€¤æ—¥, å®‰å€¤) / è¦‹ã¤ã‹ã‚‰ãªã‘ã‚Œã° None
    """
    atr20 = _atr20(high, low, close)
    pivot_high_idx = None
    pivot_high_val = None
    last_swing_low = None

    for i in range(len(close)):
        h, l = float(high.iloc[i]), float(low.iloc[i])
        if pivot_high_val is None or h > pivot_high_val:
            pivot_high_val = h
            pivot_high_idx = i

        drop_pct = (pivot_high_val - l) / pivot_high_val if pivot_high_val else 0.0
        atr_ok = (pivot_high_val - l) >= (atr_mult * float(atr20.iloc[i] if pd.notna(atr20.iloc[i]) else 0.0))
        if drop_pct >= thresh_pct or atr_ok:
            seg = low.iloc[pivot_high_idx:i+1]
            if not seg.empty:
                j = seg.idxmin()
                last_swing_low = (j, float(low.loc[j]))
            pivot_high_val = h
            pivot_high_idx = i

    return last_swing_low

def compute_one_ticker(close_s: pd.Series, high_s: pd.Series, low_s: pd.Series, window_days=60):
    """
    æ–°ä»•æ§˜ï¼š
      â‘  ä¸Šæ˜‡ãƒˆãƒ¬ãƒ³ãƒ‰ï¼ˆSMA25>SMA75ã€ä¸¡è€…ã®å‚¾ã>=0ï¼‰
      â‘¡ ç›´è¿‘ã‚¹ã‚¤ãƒ³ã‚°ãƒ­ãƒ¼ï¼ˆZigZagé¢¨ï¼‰ã‚’ç‰¹å®š
      â‘¢ Peakâ†’Lowã®â€œæ·±ã•â€ ä¸‹é™ï¼ˆ% ã¾ãŸã¯ ATR20Ã—multï¼‰
      â‘£ Swing Low ãŒ SMA25 ã«ã‚¿ãƒƒãƒï¼ˆÂ±SMA_TOUCH_TOLï¼‰or ã„ã£ãŸã‚“å‰²ã£ã¦å³å›å¾©
      â‘¤ Low ã‹ã‚‰ã®çµŒéæ—¥æ•° >= DAYS_SINCE_LOW_MIN
      â‘¥ åç™ºç‡ REBOUND_MINã€œREBOUND_MAX
    æº€ãŸã›ã° dict ã‚’è¿”ã™ã€‚æº€ãŸã•ãªã‘ã‚Œã° Noneã€‚
    """
    if close_s is None or close_s.empty:
        return None

    # å¯¾è±¡çª“ã®åˆ‡ã‚Šå‡ºã—ï¼ˆNaNè½ã¨ã—ï¼†æ•´åˆ—ï¼‰
    close = close_s.dropna().iloc[-window_days:]
    high  = high_s.reindex(close.index).ffill()
    low   = low_s.reindex(close.index).ffill()
    if len(close) < max(30, window_days // 2):
        return None

    # ãƒˆãƒ¬ãƒ³ãƒ‰ï¼šSMA25 / SMA75 ã¨å‚¾ã
    sma25 = close.rolling(25, min_periods=25).mean()
    sma75 = close.rolling(75, min_periods=75).mean()
    if sma25.isna().all() or sma75.isna().all():
        return None
    if not (sma25.iloc[-1] > sma75.iloc[-1]):
        return None
    slope25 = sma25.iloc[-1] - (sma25.iloc[-6] if len(sma25.dropna()) >= 6 else sma25.iloc[-1])
    slope75 = sma75.iloc[-1] - (sma75.iloc[-6] if len(sma75.dropna()) >= 6 else sma75.iloc[-1])
    if slope25 <= 0 or slope75 < 0:
        return None

    # ç›´è¿‘ã‚¹ã‚¤ãƒ³ã‚°ãƒ­ãƒ¼ï¼ˆZigZagï¼‰
    swing = _zigzag_last_swing_low(high, low, close, DEPTH_MIN_PCT, ATR20_MULT_MIN)
    if swing is None:
        return None
    low_date, low_val = swing
    if low_date not in close.index:
        return None

    # Peakï¼ˆlow_date ä»¥å‰ã®é«˜å€¤ï¼‰
    before = close.loc[:low_date]
    if before.empty:
        return None
    peak_val = float(before.max())
    peak_date = before.idxmax()

    # æŠ¼ã—ã®æ·±ã•ï¼ˆ%ã¨ATRã®ä¸¡æ¡ä»¶ï¼‰
    drop_pct = (peak_val - low_val) / peak_val if peak_val else 0.0
    if drop_pct < DEPTH_MIN_PCT:
        return None
    atr20 = _atr20(high, low, close)
    atr_ok = (peak_val - low_val) >= (ATR20_MULT_MIN * float(atr20.loc[low_date] if (low_date in atr20.index and pd.notna(atr20.loc[low_date])) else 0.0))
    if not atr_ok:
        return None

    # SMA25 ã‚¿ãƒƒãƒï¼ˆÂ±è¨±å®¹ or ä¸€æ™‚å‰²ã‚Œâ†’å›å¾©ï¼‰
    sma25_at_low = float(sma25.loc[low_date]) if (low_date in sma25.index and pd.notna(sma25.loc[low_date])) else None
    if sma25_at_low is None:
        return None
    touch_ok = abs(low_val - sma25_at_low) / sma25_at_low <= SMA_TOUCH_TOL or (low_val <= sma25_at_low <= float(close.iloc[-1]))
    if not touch_ok:
        return None

    # åç™ºãƒ»çµŒéæ—¥æ•°
    latest = float(close.iloc[-1])
    prev   = float(close.iloc[-2]) if len(close) >= 2 else np.nan
    days_since_low = (close.index[-1] - low_date).days
    if days_since_low < DAYS_SINCE_LOW_MIN:
        return None
    rebound_pct = (latest / low_val) - 1.0
    if not (REBOUND_MIN <= rebound_pct <= REBOUND_MAX):
        return None

    # ç›®æ¨™å€¤ï¼šLowä»¥é™ã®é«˜å€¤ï¼ˆç„¡ã‘ã‚Œã°çª“å†…é«˜å€¤ï¼‰
    after = close.loc[low_date:]
    target = float(after.max()) if not after.empty else float(close.max())
    expected_rise_pct = (target / latest - 1.0) * 100.0 if latest > 0 else None

    # â† ã“ã“ãŒé‡è¦ï¼šnotify ãŒå‚ç…§ã™ã‚‹ã‚­ãƒ¼åã«å®Œå…¨æº–æ‹ 
    return {
        "Ticker": close_s.name,
        "Peak_Date": peak_date.to_pydatetime().date(),
        "Peak_High": round(peak_val, 2),
        "Pullback_Date": low_date.to_pydatetime().date(),
        "Pullback_Low": round(low_val, 2),
        "Latest_Date": close.index[-1].to_pydatetime().date(),
        "Latest_Close": round(latest, 2),
        "Prev_Close": round(prev, 2) if not np.isnan(prev) else np.nan,
        "Expected_Upper": round(target, 2),
        "Expected_Rise_%": round(expected_rise_pct, 2) if expected_rise_pct is not None else np.nan,
        # å‚è€ƒæƒ…å ±ï¼ˆé€šçŸ¥ã§ä½¿ã‚ãªã„ãŒãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
        "Drop_From_Peak_%": round(drop_pct * 100.0, 2),
        "Rebound_From_Low_%": round(rebound_pct * 100.0, 2),
        "Days_Since_Pullback": int(days_since_low),
        "SMA25": round(float(sma25.iloc[-1]), 2) if pd.notna(sma25.iloc[-1]) else np.nan,
        "SMA75": round(float(sma75.iloc[-1]), 2) if pd.notna(sma75.iloc[-1]) else np.nan,
        "Window": int(window_days),
    }

def find_pullback_candidates(close_df: pd.DataFrame, high_df: pd.DataFrame, low_df: pd.DataFrame, window_days=30):
    rows = []
    for ticker in close_df.columns:
        res = compute_one_ticker(close_df[ticker], high_df[ticker], low_df[ticker], window_days=window_days)
        if res:
            rows.append(res)
    if not rows:
        return pd.DataFrame()
    df = pd.DataFrame(rows)
    df = df.sort_values("Expected_Rise_%", ascending=False).reset_index(drop=True)
    return df

# ===== ãƒãƒ£ãƒ¼ãƒˆç”»åƒä½œæˆï¼ˆè¸è¥²ï¼‰ =====
def save_chart_image_from_raw(raw_df, ticker: str, out_dir="charts"):
    """
    raw_df: yf.download(..., group_by='column') ã® MultiIndex DataFrame
    """
    need_cols = ["Open", "High", "Low", "Close", "Volume"]
    try:
        use = raw_df.loc[:, [(c, ticker) for c in need_cols]].copy()
    except Exception:
        return None
    if use.empty:
        return None
    use.columns = need_cols
    use = use.dropna()
    if use.empty:
        return None

    os.makedirs(out_dir, exist_ok=True)
    out_path = os.path.join(out_dir, f"{ticker}.png")
    mpf.plot(
        use,
        type="candle",
        mav=(5, 25, 75),
        volume=True,
        style="yahoo",
        savefig=dict(fname=out_path, dpi=140, bbox_inches="tight")
    )
    return out_path

# ===== åç§°è¾æ›¸ =====
def build_ticker_name_map(tickers):
    # æ—¢å­˜ã®ã‚°ãƒ­ãƒ¼ãƒãƒ«è¾æ›¸ã‚’å‚ç…§ã€‚ç„¡ã‘ã‚Œã°ç©ºæ–‡å­—ã€‚
    return {t: ticker_name_map.get(t, "") for t in tickers}

# ===== ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ =====
def run_pipeline():
    tickers = load_tickers()
    raw, close, high, low = fetch_market_data(tickers, lookback_days=DEFAULT_LOOKBACK_DAYS)

    # 60æ—¥ãƒ»30æ—¥ã§æŠ½å‡º â†’ ãƒãƒ¼ã‚¸ï¼ˆåŒä¸€ãƒ†ã‚£ãƒƒã‚«ãƒ¼ã¯ 'Expected_Rise_%' ãŒå¤§ãã„æ–¹ã‚’æ¡ç”¨ï¼‰
    rs = []
    for w in (60, 30):
        df = find_pullback_candidates(close, high, low, window_days=w)
        if not df.empty:
            df["Window"] = w
            rs.append(df)

    if not rs:
        return pd.DataFrame(), raw, {}

    cat = (
        pd.concat(rs, ignore_index=True)
          .sort_values(["Ticker", "Expected_Rise_%"], ascending=[True, False])
    )
    best = (
        cat.groupby("Ticker", as_index=False)
           .first()
           .sort_values("Expected_Rise_%", ascending=False)
           .reset_index(drop=True)
    )

    name_map = build_ticker_name_map(best["Ticker"].tolist())
    return best, raw, name_map
# ===== é€šçŸ¥ï¼ˆDiscordç‰ˆï¼‰ =====
def notify(best_df: pd.DataFrame, raw_df, ticker_name_map: dict, top_n=TOP_N):
    if best_df is None or best_df.empty:
        discord_send_content("ã€æŠ¼ã—ç›®ã‚¹ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã€‘æœ¬æ—¥ã¯æŠ½å‡ºãªã—ã§ã—ãŸã€‚")
        return

    header = (
        f"â˜…â˜…â˜…â˜…â˜…ã€æŠ¼ã—ç›®ã€‘â˜…â˜…â˜…â˜…â˜… {now_jst().strftime('%m/%d %H:%M')}\n"
        f"æŠ½å‡º: {len(best_df)} éŠ˜æŸ„ï¼ˆé‡è¤‡çµ±åˆï¼‰\n"
        f"æ¡ä»¶: {REBOUND_MAX*100:.0f}%â‰¥åç™ºâ‰¥{REBOUND_MIN*100:.0f}%ãƒ»ä¸‹è½â‰¤{DROP_MAX:.0f}%ãƒ»SMA25ä¸Šãƒ»æœŸå¾…â‰¥{EXPECTED_RISE_MIN:.0f}%ãƒ»{DAYS_SINCE_LOW_MIN}æ—¥çµŒé\n"
        f"------------------------------"
    )
    send_long_text(header)

    for _, r in best_df.head(top_n).iterrows():
        ticker = str(r["Ticker"])
        name = ticker_name_map.get(ticker, "")
        upper  = r.get("Expected_Upper")
        latest = r.get("Latest_Close")
        low    = r.get("Pullback_Low")
        rise_p = r.get("Expected_Rise_%")
        prev   = r.get("Prev_Close")

        def fnum(x):
            try: return f"{float(x):,.0f}"
            except: return "-"
        def fpct(x):
            try: return f"{float(x):.1f}%"
            except: return "-"
        def fpct_signed(x):
            try:
                x = float(x)
                if not np.isfinite(x): return "-"
                return f"{x:+.1f}%"
            except:
                return "-"

        expect_amt = (float(upper) - float(latest)) if pd.notna(upper) and pd.notna(latest) else None
        chg_pct = ((float(latest) / float(prev)) - 1.0) * 100.0 if (pd.notna(latest) and pd.notna(prev) and float(prev) != 0.0) else None
        bot_pct = ((float(latest) / float(low)) - 1.0) * 100.0 if (pd.notna(latest) and pd.notna(low) and float(low) != 0.0) else None

        pull_date = r.get("Pullback_Date")
        pull_str = pull_date.strftime("%m/%d") if hasattr(pull_date, "strftime") else "-"
        rsi_val = latest_rsi_from_raw(raw_df, ticker, period=14)
        rsi_str = "-" if rsi_val is None or not np.isfinite(rsi_val) else f"{rsi_val:.0f}"

        # ãƒ†ã‚­ã‚¹ãƒˆ 5è¡Œï¼ˆcontentï¼‰
        line1 = f"{ticker} {name}".rstrip()
        line2 = f"åº•æ—¥ {pull_str}"
        line3 = f"â†— {fpct(rise_p)}   ğŸ¯ ä¸Š {fnum(upper)}   ä¸‹ {fnum(low)}"
        line4 = f"ä»Š {fnum(latest)}   ğŸ¯ æœŸå¾… {fnum(expect_amt)}  RSI {rsi_str}"
        line5 = f"å¤‰å‹•ç‡ {fpct_signed(chg_pct)}   åº•å€¤æ¯”è¼ƒ {fpct_signed(bot_pct)}"
        msg = "\n".join([line1, line2, line3, line4, line5])
        send_long_text(msg)

        # ãƒãƒ£ãƒ¼ãƒˆç”»åƒï¼ˆEmbed or æ·»ä»˜ï¼‰
        img_path = save_chart_image_from_raw(raw_df, ticker, out_dir="charts")
        title = f"{ticker} {name}".strip()
        desc = f"Window: best / æœŸå¾…ä¸Šæ˜‡ {fpct(rise_p)}"
        fields = [
            {"name": "Pullback", "value": f"{pull_str}",     "inline": True},
            {"name": "Latest",   "value": f"{fnum(latest)}", "inline": True},
            {"name": "Target",   "value": f"{fnum(upper)}",  "inline": True},
        ]

        if img_path:
            if PUBLIC_BASE_URL:
                public_url = f"{PUBLIC_BASE_URL}/{os.path.basename(img_path)}"
                discord_send_embed(
                    title=title,
                    description=desc,
                    image_url=public_url,
                    fields=fields,
                )
            else:
                discord_send_image_file(
                    file_path=img_path,
                    title=title,
                    description=desc,
                    fields=fields,
                )
        else:
            discord_send_embed(
                title=title,
                description=desc,
                fields=fields,
            )


def main():
    now = now_jst()
    force = os.getenv("FORCE_RUN") == "1"

    if not force and is_weekend(now):
        print(f"[SKIP] {now:%F %R} é€±æœ«ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—ï¼ˆFORCE_RUN=1ã§å®Ÿè¡Œå¯èƒ½ï¼‰")
        return

    best, raw, name_map = run_pipeline()
    notify(best, raw, name_map, top_n=TOP_N)


if __name__ == "__main__":
    main()
