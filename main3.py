# -*- coding: utf-8 -*-
# main3.py â€” æŠ¼ã—ç›®æŠ½å‡º â†’ Discord Webhook ã«ã€Œãƒ†ã‚­ã‚¹ãƒˆï¼ˆåˆ†å‰²ï¼‰â†’ ãƒãƒ£ãƒ¼ãƒˆç”»åƒ(Embed)ã€é€£ç¶šé€ä¿¡
# ä¾å­˜: pandas, numpy, yfinance, mplfinance, requests
#
# ç’°å¢ƒå¤‰æ•°:
#   DISCORD_WEBHOOK_URL  (å¿…é ˆ)
#   PUBLIC_BASE_URL      (ä»»æ„; ä¾‹: https://<user>.github.io/charts ãªã© / æœ«å°¾ã‚¹ãƒ©ç„¡ã—ã§ã‚‚OK)
#   ï¼ˆä»»æ„ï¼‰FORCE_RUN=1 ã§é€±æœ«ã‚¹ã‚­ãƒƒãƒ—ç„¡åŠ¹åŒ–
#   ï¼ˆä»»æ„ï¼‰TICKERS_CSV=./tickers.csv  (Tickeråˆ—/Codeåˆ—/Symbolåˆ—ã‚’å«ã‚€CSV)
#   ï¼ˆä»»æ„ï¼‰LOOKBACK_DAYS=180
#

import os
import sys
import math
from datetime import datetime, timedelta
import requests
import numpy as np
import pandas as pd
import yfinance as yf
import mplfinance as mpf

# ===== è¨­å®šï¼ˆå¿…è¦ã«å¿œã˜ã¦å¤‰æ›´ï¼‰ =====
TZ_OFFSET = 9  # JST
REBOUND_MIN = 1.0       # åç™ºç‡ >= 1%
REBOUND_MAX = 4.0       # åç™ºç‡ <= 4%
DROP_MAX = 15.0         # ãƒ”ãƒ¼ã‚¯ã‹ã‚‰ã®è¨±å®¹ä¸‹è½ç‡ <= 15%
DAYS_SINCE_MIN = 2      # æŠ¼ã—ç›®ã‹ã‚‰æœ€æ–°ã¾ã§ã®å–¶æ¥­æ—¥æ•° >= 2
EXPECTED_RISE_MIN = 3.0 # æœŸå¾…ä¸Šæ˜‡ç‡ >= 3%
SMA_WINDOW = 25
TOP_N = 15              # é€ä¿¡ä¸Šé™
DEFAULT_LOOKBACK_DAYS = int(os.getenv("LOOKBACK_DAYS", "180"))

# ===== Discord Webhook =====
DISCORD_WEBHOOK_URL = os.environ.get("DISCORD_WEBHOOK_URL", "").strip()
PUBLIC_BASE_URL = os.environ.get("PUBLIC_BASE_URL", "").rstrip("/")

if not DISCORD_WEBHOOK_URL:
    print("[ERROR] Set DISCORD_WEBHOOK_URL env.", file=sys.stderr)

# ===== ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ =====
def now_jst():
    return datetime.utcnow() + timedelta(hours=TZ_OFFSET)

def is_weekend(dt: datetime) -> bool:
    # åœŸæ—¥ã‚¹ã‚­ãƒƒãƒ—ï¼ˆæ—¥æœ¬ã®ç¥æ—¥ã¯è€ƒæ…®ã—ãªã„ï¼‰
    return dt.weekday() >= 5

def chunk_text(text: str, limit: int = 1900):
    """
    Discordã®contentåˆ¶é™(2000æ–‡å­—)ã‚’è€ƒæ…®ã—ã¦åˆ†å‰²ï¼ˆä½™è£•1900ï¼‰ã€‚
    æ”¹è¡Œå˜ä½ã§åˆ†å‰²ã€‚é•·ã™ãã‚‹1è¡Œã¯å¼·åˆ¶çš„ã«åˆ‡ã‚‹ã€‚
    """
    out, buf, size = [], [], 0
    for line in text.splitlines():
        if len(line) > limit:
            # 1è¡ŒãŒè¶…é•·ã„ã¨ãã¯å¼·åˆ¶åˆ†å‰²
            while len(line) > limit:
                part = line[:limit]
                line = line[limit:]
                if buf:
                    out.append("\n".join(buf))
                    buf, size = [], 0
                out.append(part)
            if line == "":
                continue
        add = len(line) + 1
        if size + add > limit:
            out.append("\n".join(buf))
            buf, size = [line], len(line) + 1
        else:
            buf.append(line)
            size += add
    if buf:
        out.append("\n".join(buf))
    return out


# ===== æ—¥çµŒ225ãƒ†ã‚£ãƒƒã‚«ãƒ¼ =====
nikkei225_tickers = [ '4151.T','4502.T','4503.T','4506.T','4507.T','4519.T','4523.T','4568.T','4578.T','6479.T','6501.T','6503.T','6504.T','6506.T','6526.T','6594.T','6645.T','6674.T','6701.T','6702.T','6723.T','6724.T','6752.T','6753.T','6758.T','6762.T','6770.T','6841.T','6857.T','6861.T','6902.T','6920.T','6952.T','6954.T','6971.T','6976.T','6981.T','7735.T','7751.T','7752.T','8035.T','7201.T','7202.T','7203.T','7205.T','7211.T','7261.T','7267.T','7269.T','7270.T','7272.T','4543.T','4902.T','6146.T','7731.T','7733.T','7741.T','7762.T','9432.T','9433.T','9434.T','9613.T','9984.T','5831.T','7186.T','8304.T','8306.T','8308.T','8309.T','8316.T','8331.T','8354.T','8411.T','8253.T','8591.T','8697.T','8601.T','8604.T','8630.T','8725.T','8750.T','8766.T','8795.T','1332.T','2002.T','2269.T','2282.T','2501.T','2502.T','2503.T','2801.T','2802.T','2871.T','2914.T','3086.T','3092.T','3099.T','3382.T','7453.T','8233.T','8252.T','8267.T','9843.T','9983.T','2413.T','2432.T','3659.T','4307.T','4324.T','4385.T','4661.T','4689.T','4704.T','4751.T','4755.T','6098.T','6178.T','7974.T','9602.T','9735.T','9766.T','1605.T','3401.T','3402.T','3861.T','3405.T','3407.T','4004.T','4005.T','4021.T','4042.T','4043.T','4061.T','4063.T','4183.T','4188.T','4208.T','4452.T','4901.T','4911.T','6988.T','5019.T','5020.T','5101.T','5108.T','5201.T','5214.T','5233.T','5301.T','5332.T','5333.T','5401.T','5406.T','5411.T','3436.T','5706.T','5711.T','5713.T','5714.T','5801.T','5802.T','5803.T','2768.T','8001.T','8002.T','8015.T','8031.T','8053.T','8058.T','1721.T','1801.T','1802.T','1803.T','1808.T','1812.T','1925.T','1928.T','1963.T','5631.T','6103.T','6113.T','6273.T','6301.T','6302.T','6305.T','6326.T','6361.T','6367.T','6471.T','6472.T','6473.T','7004.T','7011.T','7013.T','7012.T','7832.T','7911.T','7912.T','7951.T','3289.T','8801.T','8802.T','8804.T','8830.T','9001.T','9005.T','9007.T','9008.T','9009.T','9020.T','9021.T','9022.T','9064.T','9147.T','9101.T','9104.T','9107.T','9201.T','9202.T','9301.T','9501.T','9502.T','9503.T','9531.T','9532.T' ]

# ===== çŸ­ç¸®åãƒãƒƒãƒ— =====
ticker_name_map = {
    "1332.T": "æ—¥æ°´", "1333.T": "ãƒãƒ«ãƒãƒ‹ãƒãƒ­", "1605.T": "INPEX", "1801.T": "å¤§æˆå»º",
    "1802.T": "æ¸…æ°´å»º", "1803.T": "é£›å³¶å»º", "1808.T": "é•·è°·å·¥", "1812.T": "é¹¿å³¶",
    "1925.T": "å¤§å’Œãƒã‚¦ã‚¹", "1928.T": "ç©æ°´ãƒã‚¦ã‚¹", "1963.T": "æ—¥æ®HD", "2002.T": "æ—¥æ¸…ç²‰G",
    "2269.T": "æ˜æ²»HD", "2282.T": "æ—¥æœ¬ãƒãƒ ", "2413.T": "ã‚¨ãƒ ã‚¹ãƒªãƒ¼", "2432.T": "DeNA",
    "2501.T": "ã‚µãƒƒãƒãƒ­HD", "2502.T": "ã‚¢ã‚µãƒ’GHD", "2503.T": "ã‚­ãƒªãƒ³HD", "2768.T": "åŒæ—¥",
    "2801.T": "ã‚­ãƒƒã‚³ãƒãƒ³", "2802.T": "å‘³ã®ç´ ", "2871.T": "ãƒ‹ãƒãƒ¬ã‚¤", "2914.T": "JT",
    "3086.T": "Jãƒ•ãƒ­ãƒ³ãƒˆ", "3092.T": "ZOZO", "3099.T": "ä¸‰è¶Šä¼Šå‹¢ä¸¹", "3382.T": "ã‚»ãƒ–ãƒ³&ã‚¢ã‚¤",
    "3401.T": "å¸äºº", "3402.T": "æ±ãƒ¬", "3405.T": "ã‚¯ãƒ©ãƒ¬", "3407.T": "æ—­åŒ–æˆ",
    "3436.T": "SUMCO", "3861.T": "ç‹å­HD", "4004.T": "æ˜­é›»å·¥", "4005.T": "ä½å‹åŒ–å­¦",
    "4021.T": "æ—¥ç”£åŒ–", "4042.T": "æ±ã‚½ãƒ¼", "4043.T": "ãƒˆã‚¯ãƒ¤ãƒ", "4061.T": "é›»åŒ–",
    "4063.T": "ä¿¡è¶ŠåŒ–", "4183.T": "ä¸‰äº•åŒ–å­¦", "4188.T": "ä¸‰è±ã‚±ãƒŸHD", "4208.T": "UBE",
    "4452.T": "èŠ±ç‹", "4502.T": "æ­¦ç”°è–¬å“", "4503.T": "ã‚¢ã‚¹ãƒ†ãƒ©ã‚¹", "4506.T": "å¤§æ—¥æœ¬ä½å‹",
    "4507.T": "å¡©é‡ç¾©", "4519.T": "ä¸­å¤–è£½è–¬", "4523.T": "ã‚¨ãƒ¼ã‚¶ã‚¤", "4543.T": "ãƒ†ãƒ«ãƒ¢",
    "4568.T": "ç¬¬ä¸€ä¸‰å…±", "4578.T": "å¤§å¡šHD", "4661.T": "OLC", "4689.T": "ZHD",
    "4704.T": "ãƒˆãƒ¬ãƒ³ãƒ‰", "4751.T": "ã‚µã‚¤ãƒãƒ¼", "4755.T": "æ¥½å¤©G", "4901.T": "å¯Œå£«ãƒ•ã‚¤ãƒ«ãƒ ",
    "4902.T": "ã‚³ãƒ‹ã‚«ãƒŸãƒãƒ«ã‚¿", "4911.T": "è³‡ç”Ÿå ‚", "5020.T": "ENEOS",
    "5101.T": "æ¨ªæµœã‚´ãƒ ", "5108.T": "ãƒ–ãƒªãƒ‚ã‚¹ãƒˆãƒ³", "5201.T": "AGC", "5214.T": "æ—¥é›»ç¡",
    "5233.T": "å¤ªå¹³æ´‹ã‚»ãƒ¡", "5301.T": "æ±æµ·ã‚«ãƒ¼ãƒœãƒ³", "5332.T": "TOTO", "5333.T": "æ—¥æœ¬ã‚¬ã‚¤ã‚·",
    "5401.T": "æ—¥æœ¬è£½é‰„", "5406.T": "ç¥æˆ¸è£½é‹¼", "5411.T": "JFEHD", "5706.T": "ä¸‰äº•é‡‘å±",
    "5711.T": "ä¸‰è±ãƒãƒ†", "5713.T": "ä½å‹é‡‘å±é‰±å±±", "5714.T": "DOWA", "5801.T": "å¤æ²³é›»å·¥",
    "5802.T": "ä½å‹é›»å·¥", "5803.T": "ãƒ•ã‚¸ã‚¯ãƒ©", "6098.T": "ãƒªã‚¯ãƒ«ãƒ¼ãƒˆHD", "6178.T": "æ—¥æœ¬éƒµæ”¿",
    "6273.T": "SMC", "6301.T": "ã‚³ãƒãƒ„", "6302.T": "ä½å‹é‡æ©Ÿ", "6305.T": "æ—¥ç«‹å»ºæ©Ÿ",
    "6326.T": "ã‚¯ãƒœã‚¿", "6361.T": "èåŸ", "6367.T": "ãƒ€ã‚¤ã‚­ãƒ³", "6471.T": "æ—¥ç²¾å·¥",
    "6472.T": "NTN", "6473.T": "ã‚¸ã‚§ã‚¤ãƒ†ã‚¯ãƒˆ", "6479.T": "ãƒŸãƒãƒ™ã‚¢ãƒŸãƒ„ãƒŸ", "6501.T": "æ—¥ç«‹",
    "6503.T": "ä¸‰è±é›»æ©Ÿ", "6504.T": "å¯Œå£«é›»æ©Ÿ", "6506.T": "å®‰å·é›»æ©Ÿ", "6526.T": "ã‚½ã‚·ã‚ªãƒã‚¯ã‚¹ãƒˆ",
    "6594.T": "æ—¥é›»ç”£", "6645.T": "ã‚ªãƒ ãƒ­ãƒ³", "6674.T": "ã‚¸ãƒ¼ã‚¨ã‚¹ãƒ¦ã‚¢ã‚µ", "6701.T": "NEC",
    "6702.T": "å¯Œå£«é€š", "6723.T": "ãƒ«ãƒã‚µã‚¹", "6724.T": "ã‚»ã‚¤ã‚³ãƒ¼ã‚¨ãƒ—ã‚½ãƒ³", "6752.T": "ãƒ‘ãƒŠã‚½ãƒ‹ãƒƒã‚¯",
    "6753.T": "ã‚·ãƒ£ãƒ¼ãƒ—", "6758.T": "ã‚½ãƒ‹ãƒ¼G", "6762.T": "TDK", "6770.T": "ã‚¢ãƒ«ãƒ—ã‚¹ã‚¢ãƒ«ãƒ‘",
    "6841.T": "æ¨ªæ²³é›»æ©Ÿ", "6857.T": "ã‚¢ãƒ‰ãƒ†ã‚¹ãƒˆ", "6861.T": "ã‚­ãƒ¼ã‚¨ãƒ³ã‚¹", "6902.T": "ãƒ‡ãƒ³ã‚½ãƒ¼",
    "6920.T": "ãƒ¬ãƒ¼ã‚¶ãƒ¼ãƒ†ãƒƒã‚¯", "6952.T": "ã‚«ã‚·ã‚ª", "6954.T": "ãƒ•ã‚¡ãƒŠãƒƒã‚¯", "6971.T": "äº¬ã‚»ãƒ©",
    "6976.T": "å¤ªé™½èª˜é›»", "6981.T": "æ‘ç”°è£½ä½œæ‰€", "6988.T": "æ—¥æ±é›»å·¥", "7201.T": "æ—¥ç”£è‡ª",
    "7202.T": "ã„ã™ã‚", "7203.T": "ãƒˆãƒ¨ã‚¿", "7205.T": "æ—¥é‡è‡ª", "7211.T": "ä¸‰è±è‡ª",
    "7261.T": "ãƒãƒ„ãƒ€", "7267.T": "ãƒ›ãƒ³ãƒ€", "7269.T": "ã‚¹ã‚ºã‚­", "7270.T": "SUBARU",
    "7272.T": "ãƒ¤ãƒãƒç™º", "7453.T": "è‰¯å“è¨ˆç”»", "7731.T": "ãƒ‹ã‚³ãƒ³", "7733.T": "ã‚ªãƒªãƒ³ãƒ‘ã‚¹",
    "7735.T": "ã‚¹ã‚¯ãƒªãƒ³", "7741.T": "HOYA", "7751.T": "ã‚­ãƒ¤ãƒãƒ³", "7752.T": "ãƒªã‚³ãƒ¼",
    "7762.T": "ã‚·ãƒã‚ºãƒ³", "7832.T": "ãƒãƒ³ãƒŠãƒ HD", "7911.T": "å‡¸ç‰ˆå°åˆ·", "7912.T": "å¤§æ—¥æœ¬å°åˆ·",
    "7951.T": "ãƒ¤ãƒãƒ", "7974.T": "ä»»å¤©å ‚", "8001.T": "ä¼Šè—¤å¿ ", "8002.T": "ä¸¸ç´…",
    "8015.T": "è±Šç”°é€šå•†", "8031.T": "ä¸‰äº•ç‰©ç”£", "8035.T": "æ±ã‚¨ãƒ¬ã‚¯", "8053.T": "ä½å‹å•†äº‹",
    "8058.T": "ä¸‰è±å•†äº‹", "8113.T": "ãƒ¦ãƒ‹ãƒãƒ£ãƒ¼ãƒ ", "8252.T": "ä¸¸äº•G", "8253.T": "ã‚¯ãƒ¬ã‚»ã‚¾ãƒ³",
    "8267.T": "ã‚¤ã‚ªãƒ³", "8304.T": "ã‚ãŠãã‚‰éŠ€", "8306.T": "ä¸‰è±UFJ", "8308.T": "ã‚ŠããªHD",
    "8309.T": "ä¸‰äº•ä½å‹", "8316.T": "ä¸‰äº•ä½å‹ä¿¡è¨—", "8331.T": "åƒè‘‰éŠ€", "8354.T": "ãµããŠã‹FG",
    "8411.T": "ã¿ãšã»", "8591.T": "ã‚ªãƒªãƒƒã‚¯ã‚¹", "8601.T": "å¤§å’Œè¨¼G", "8604.T": "é‡æ‘HD",
    "8630.T": "ä½å‹ä¿¡è¨—", "8697.T": "æ—¥å–æ‰€", "8725.T": "MS&AD", "8750.T": "ç¬¬ä¸€ç”Ÿå‘½",
    "8766.T": "æ±äº¬æµ·ä¸Š", "8795.T": "T&DHD", "8801.T": "ä¸‰äº•ä¸", "8802.T": "ä¸‰è±åœ°æ‰€",
    "8804.T": "æ±äº¬å»ºç‰©", "8830.T": "ä½å‹ä¸", "9001.T": "æ±æ­¦", "9005.T": "æ±æ€¥",
    "9007.T": "å°ç”°æ€¥", "9008.T": "äº¬ç‹", "9009.T": "äº¬æˆ", "9020.T": "JRæ±æ—¥æœ¬",
    "9021.T": "JRè¥¿æ—¥æœ¬", "9022.T": "JRæ±æµ·", "9064.T": "ãƒ¤ãƒãƒˆHD", "9101.T": "æ—¥æœ¬éƒµèˆ¹",
    "9104.T": "å•†èˆ¹ä¸‰äº•", "9107.T": "å·å´æ±½èˆ¹", "9147.T": "NXHD", "9201.T": "JAL",
    "9202.T": "ANAHD", "9301.T": "ä¸‰è±å€‰åº«", "9432.T": "NTT", "9433.T": "KDDI",
    "9434.T": "ã‚½ãƒ•ãƒˆãƒãƒ³ã‚¯", "9501.T": "æ±é›»HD", "9502.T": "ä¸­éƒ¨é›»", "9503.T": "é–¢è¥¿é›»",
    "9531.T": "æ±ã‚¬ã‚¹", "9532.T": "å¤§é˜ªã‚¬ã‚¹", "9602.T": "æ±å®", "9613.T": "NTTãƒ‡ãƒ¼ã‚¿",
    "9735.T": "ã‚»ã‚³ãƒ ", "9766.T": "ã‚³ãƒŠãƒŸG", "9843.T": "ãƒ‹ãƒˆãƒªHD", "9983.T": "ãƒ•ã‚¡ãƒ¼ã‚¹ãƒˆãƒªãƒ†",
    "9984.T": "ã‚½ãƒ•ãƒˆãƒãƒ³ã‚¯G",
}

# ======= RSI è¨ˆç®—ãƒ˜ãƒ«ãƒ‘ãƒ¼ï¼ˆLINEç‰ˆã‹ã‚‰è¸è¥²ï¼‰ =======
def latest_rsi_from_raw(raw_df, ticker: str, period: int = 14):
    """
    yf.download(..., group_by='column') ã®ç”Ÿãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å¯¾è±¡ãƒ†ã‚£ãƒƒã‚«ãƒ¼ã®çµ‚å€¤ã§RSI(14)ã‚’ç®—å‡ºã€‚
    å–å¾—ä¸å¯ã®å ´åˆã¯ None ã‚’è¿”ã™ã€‚
    """
    try:
        if isinstance(raw_df.columns, pd.MultiIndex):
            close = raw_df[("Close", ticker)].dropna()
        else:
            close = raw_df["Close"].dropna()
        if len(close) < period + 2:
            return None
        delta = close.diff()
        up = delta.clip(lower=0.0)
        down = (-delta).clip(lower=0.0)
        roll_up = up.rolling(period, min_periods=period).mean()
        roll_down = down.rolling(period, min_periods=period).mean()
        rs = roll_up / roll_down.replace(0, np.nan)
        rsi = 100.0 - (100.0 / (1.0 + rs))
        return float(rsi.iloc[-1])
    except Exception:
        return None

# ===== Discordé€ä¿¡ =====
def discord_send_content(msg: str):
    """
    contentã§ã‚·ãƒ³ãƒ—ãƒ«ã«é€ä¿¡ã€‚2000æ–‡å­—åˆ¶é™ã«åˆã‚ã›ã¦äº‹å‰åˆ†å‰²ã€‚
    """
    headers = {"Content-Type": "application/json"}
    for part in chunk_text(msg, limit=1900):
        payload = {"content": part}
        r = requests.post(DISCORD_WEBHOOK_URL, json=payload, headers=headers, timeout=20)
        if r.status_code >= 300:
            raise RuntimeError(f"Discord send failed: {r.status_code} {r.text}")

def discord_send_embed(title: str, description: str | None = None, image_url: str | None = None, fields: list | None = None):
    """
    Embedã§é€ä¿¡ï¼ˆãƒãƒ£ãƒ¼ãƒˆç”»åƒURLã‚’imageã«è¡¨ç¤ºï¼‰ã€‚
    """
    embed = {"title": title, "timestamp": datetime.utcnow().isoformat() + "Z"}
    if description:
        # descriptionã‚‚2000æ–‡å­—åˆ¶é™ã‚ã‚‹ãŒã€ã“ã“ã§ã¯çŸ­æ–‡æƒ³å®š
        embed["description"] = description
    if image_url:
        embed["image"] = {"url": image_url}
    if fields:
        embed["fields"] = fields

    payload = {"embeds": [embed]}
    headers = {"Content-Type": "application/json"}
    r = requests.post(DISCORD_WEBHOOK_URL, json=payload, headers=headers, timeout=20)
    if r.status_code >= 300:
        raise RuntimeError(f"Discord embed failed: {r.status_code} {r.text}")

def send_long_text(msg: str):
    # é•·æ–‡ã‚’è‡ªå‹•åˆ†å‰²ã—ã¦é€ã‚‹ï¼ˆcontentï¼‰
    discord_send_content(msg)

# ===== ãƒ‡ãƒ¼ã‚¿å–å¾— =====
def load_tickers():
    # å„ªå…ˆ: ç’°å¢ƒå¤‰æ•° TICKERS_CSV ã®CSVï¼ˆTicker/Symbol/Codeåˆ—ï¼‰
    csv_path = os.getenv("TICKERS_CSV")
    if csv_path and os.path.exists(csv_path):
        df = pd.read_csv(csv_path)
        col = None
        for c in df.columns:
            if c.lower() in ("ticker", "symbol", "code"):
                col = c
                break
        if col:
            tickers = [str(x).strip() for x in df[col].dropna().unique().tolist()]
            if tickers:
                return tickers
    return nikkei225_tickers

def fetch_market_data(tickers, lookback_days=DEFAULT_LOOKBACK_DAYS):
    """
    Colabã®å®‰å®šãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç§»æ¤ï¼š
    - ã¾ã¨ã‚ã¦downloadï¼ˆthreads=Falseï¼‰
    - MultiIndexåˆ—ã‹ã‚‰ stack(dropna=False) ã§ç¸¦æŒã¡åŒ–
    - åˆ—ã‚’æƒãˆã¦ã‹ã‚‰ pivotï¼ˆæ¨ªå±•é–‹ï¼‰
    - Closeã‚’â€œåˆ†æç”¨ã®çµ‚å€¤â€ã¨ã—ã¦Adj_Closeåˆ—åã«çµ±ä¸€
    """
    end_dt = (now_jst().date() + timedelta(days=1)).isoformat()
    start_dt = (now_jst().date() - timedelta(days=lookback_days)).isoformat()

    # ã¾ã¨ã‚ã¦DLï¼ˆã“ã“ã‚’Colabã¨åŒã˜æ–¹é‡ã«ï¼‰
    raw = yf.download(
        tickers=tickers,
        start=start_dt,
        end=end_dt,
        interval="1d",
        auto_adjust=False,   # Colabã¨åˆã‚ã›ã‚‹ï¼ˆClose=é€šå¸¸ã®çµ‚å€¤ï¼‰
        progress=False,
        threads=False        # â˜…å®‰å®šå„ªå…ˆ
        # group_by ã¯æ˜ç¤ºã—ãªã„ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ columnï¼‰
    )

    if raw is None or len(raw) == 0:
        raise RuntimeError("yfinance returned empty frame for all tickers.")

    # --- ç¸¦æŒã¡ â†’ çµåˆï¼ˆColabã®ã‚„ã‚Šæ–¹ï¼‰ ---
    # â€» dropna=False ã§æ¬ æã‚‚ä¿æŒã—ã€å¾Œæ®µã§å¿…è¦åˆ—ã ã‘ã‚’å®‰å…¨ã«çµ„ã¿ç«‹ã¦
    try:
        s_close = raw["Close"].stack(dropna=False)
        s_open  = raw["Open" ].stack(dropna=False)
        s_high  = raw["High" ].stack(dropna=False)
        s_low   = raw["Low"  ].stack(dropna=False)
        s_vol   = raw["Volume"].stack(dropna=False)
    except Exception as e:
        raise RuntimeError(f"stack failed: {e}")

    vdf = pd.concat(
        {
            "Adj_Close": s_close,    # åˆ†æå´ã¯â€œçµ‚å€¤â€ã¨ã—ã¦ã“ã®åˆ—åã«å¯„ã›ã‚‹ï¼ˆColabæº–æ‹ ï¼‰
            "Open":      s_open,
            "high":      s_high,
            "low":       s_low,
            "volume":    s_vol,
        },
        axis=1
    ).reset_index()
    vdf.columns = ["Date", "Ticker", "Adj_Close", "Open", "high", "low", "volume"]

    # --- æ¨ªå±•é–‹ï¼šåˆ†æç”¨DataFrameï¼ˆclose_df / high_df / low_dfï¼‰ ---
    vdf["Date"] = pd.to_datetime(vdf["Date"])
    vdf = vdf.sort_values(["Ticker", "Date"])

    close_df = vdf.pivot(index="Date", columns="Ticker", values="Adj_Close").sort_index()
    high_df  = vdf.pivot(index="Date", columns="Ticker", values="high").sort_index()
    low_df   = vdf.pivot(index="Date", columns="Ticker", values="low").sort_index()

    # main3ã®ä»–å‡¦ç†ã¨äº’æ›ã«ã™ã‚‹ãŸã‚ raw ã‚‚è¿”ã™ï¼ˆãƒãƒ£ãƒ¼ãƒˆç”Ÿæˆç”¨ï¼‰
    # ãƒãƒ£ãƒ¼ãƒˆã¯ (Open,High,Low,Close,Volume) ã‚’è¦æ±‚ã™ã‚‹ãŸã‚ã€ç°¡æ˜“ãƒªãƒ“ãƒ«ãƒ‰
    # -> ç”Ÿã® raw ã‚’ãã®ã¾ã¾è¿”ã—ã¦OKï¼ˆMultiIndexã® (field,ticker) æ§‹é€ ï¼‰
    #    ä»¥é™ã® save_chart_image_from_raw ã§å‚ç…§ã§ãã¾ã™
    return raw, close_df, high_df, low_df



# ===== æŠ¼ã—ç›®æŠ½å‡ºï¼ˆå³ã—ã„æ¡ä»¶ãƒ»LINEç‰ˆè¸è¥²ï¼‰ =====
def rolling_sma(series: pd.Series, window=SMA_WINDOW):
    return series.rolling(window, min_periods=window).mean()

def compute_one_ticker(close_s: pd.Series, high_s: pd.Series, low_s: pd.Series, window_days=30):
    try:
        close_s = close_s.dropna()
        high_s = high_s.reindex_like(close_s).dropna()
        low_s  = low_s.reindex_like(close_s).dropna()
        if len(close_s) < max(SMA_WINDOW, window_days) + 2:
            return None

        # å¯¾è±¡æœŸé–“
        look = close_s.iloc[-window_days:]
        look_high = high_s.loc[look.index]
        look_low  = low_s.loc[look.index]
        if look_high.empty or look_low.empty:
            return None

        # ãƒ”ãƒ¼ã‚¯ï¼ˆæœŸé–“å†…ã®æœ€é«˜å€¤ï¼‰â€” æœ€çµ‚è¡Œï¼ˆå½“æ—¥è¶³ï¼‰ã‚’æ¢ç´¢ã‹ã‚‰é™¤å¤–
        look_high_use = look_high.iloc[:-1] if len(look_high) > 1 else look_high
        if look_high_use.empty:
            return None
        peak_idx = look_high_use.idxmax()
        peak_val = float(look_high_use.loc[peak_idx])

        # ãƒ”ãƒ¼ã‚¯å¾Œã®æœ€å®‰å€¤ï¼ˆãªã‘ã‚Œã°é™¤å¤–ï¼‰
        after_peak = look_low.loc[look_low.index > peak_idx]
        if after_peak.empty:
            return None
        pull_idx = after_peak.idxmin()
        pull_val = float(after_peak.loc[pull_idx])


        latest_idx = close_s.index[-1]
        latest_val = float(close_s.iloc[-1])
        prev_val = float(close_s.iloc[-2]) if len(close_s) >= 2 else np.nan

        sma25 = float(rolling_sma(close_s).iloc[-1]) if len(close_s) >= SMA_WINDOW else np.nan

        rebound_pct = (latest_val / pull_val - 1.0) * 100.0
        drop_pct = (1.0 - latest_val / peak_val) * 100.0
        expected_upper = peak_val
        expected_rise_pct = (expected_upper / latest_val - 1.0) * 100.0
        days_since_pull = (close_s.index.get_loc(latest_idx) - close_s.index.get_loc(pull_idx))

        conds = [
            rebound_pct >= REBOUND_MIN,
            rebound_pct <= REBOUND_MAX,
            drop_pct <= DROP_MAX,
            days_since_pull >= DAYS_SINCE_MIN,
            not math.isnan(sma25) and latest_val >= sma25,
            expected_rise_pct >= EXPECTED_RISE_MIN,
            latest_val >= pull_val,
        ]
        if not all(conds):
            return None

        return {
            "Ticker": close_s.name,
            "Peak_Date": peak_idx.date(),
            "Peak_High": round(peak_val, 2),
            "Pullback_Date": pull_idx.date(),
            "Pullback_Low": round(pull_val, 2),
            "Latest_Date": latest_idx.date(),
            "Latest_Close": round(latest_val, 2),
            "Prev_Close": round(prev_val, 2) if not np.isnan(prev_val) else np.nan,
            "Return_%": round(expected_rise_pct, 2),
            "Rebound_From_Low_%": round(rebound_pct, 2),
            "Drop_From_Peak_%": round(drop_pct, 2),
            "Days_Since_Pullback": int(days_since_pull),
            "SMA25": round(sma25, 2) if not math.isnan(sma25) else np.nan,
            "Expected_Upper": round(expected_upper, 2),
            "Expected_Rise_%": round(expected_rise_pct, 2),
        }
    except Exception as e:
        print(f"[WARN] compute_one_ticker failed for {close_s.name}: {e}", file=sys.stderr)
        return None

def find_pullback_candidates(close_df: pd.DataFrame, high_df: pd.DataFrame, low_df: pd.DataFrame, window_days=30):
    rows = []
    for ticker in close_df.columns:
        res = compute_one_ticker(close_df[ticker], high_df[ticker], low_df[ticker], window_days=window_days)
        if res:
            rows.append(res)
    if not rows:
        return pd.DataFrame()
    df = pd.DataFrame(rows)
    df = df.sort_values("Return_%", ascending=False).reset_index(drop=True)
    return df

# ===== ãƒãƒ£ãƒ¼ãƒˆç”»åƒä½œæˆï¼ˆè¸è¥²ï¼‰ =====
def save_chart_image_from_raw(raw_df, ticker: str, out_dir="charts"):
    """
    raw_df: yf.download(..., group_by='column') ã® MultiIndex DataFrame
    """
    need_cols = ["Open", "High", "Low", "Close", "Volume"]
    try:
        use = raw_df.loc[:, [(c, ticker) for c in need_cols]].copy()
    except Exception:
        return None
    if use.empty:
        return None
    use.columns = need_cols
    use = use.dropna()
    if use.empty:
        return None

    os.makedirs(out_dir, exist_ok=True)
    out_path = os.path.join(out_dir, f"{ticker}.png")
    mpf.plot(
        use,
        type="candle",
        mav=(5, 25, 75),
        volume=True,
        style="yahoo",
        savefig=dict(fname=out_path, dpi=140, bbox_inches="tight")
    )
    return out_path

# ===== åç§°è¾æ›¸ =====
def build_ticker_name_map(tickers):
    # æ—¢å­˜ã®ã‚°ãƒ­ãƒ¼ãƒãƒ«è¾æ›¸ã‚’å‚ç…§ã€‚ç„¡ã‘ã‚Œã°ç©ºæ–‡å­—ã€‚
    return {t: ticker_name_map.get(t, "") for t in tickers}

# ===== ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ =====
def run_pipeline():
    tickers = load_tickers()
    raw, close, high, low = fetch_market_data(tickers, lookback_days=DEFAULT_LOOKBACK_DAYS)

    # 30æ—¥ãƒ»14æ—¥ã§æŠ½å‡º â†’ ãƒãƒ¼ã‚¸ï¼ˆåŒä¸€ãƒ†ã‚£ãƒƒã‚«ãƒ¼ã¯ 'Return_%' ãŒå¤§ãã„æ–¹ã‚’æ¡ç”¨ï¼‰
    rs = []
    for w in (30, 14):
        df = find_pullback_candidates(close, high, low, window_days=w)
        if not df.empty:
            df["Window"] = w
            rs.append(df)

    if not rs:
        return pd.DataFrame(), raw, {}

    cat = pd.concat(rs, ignore_index=True).sort_values(["Ticker", "Return_%"], ascending=[True, False])
    best = cat.groupby("Ticker", as_index=False).first().sort_values("Return_%", ascending=False).reset_index(drop=True)
    name_map = build_ticker_name_map(best["Ticker"].tolist())
    return best, raw, name_map

# ===== é€šçŸ¥ï¼ˆDiscordç‰ˆï¼‰ =====
def notify(best_df: pd.DataFrame, raw_df, ticker_name_map: dict, top_n=TOP_N):
    if best_df is None or best_df.empty:
        discord_send_content("ã€æŠ¼ã—ç›®ã‚¹ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã€‘æœ¬æ—¥ã¯æŠ½å‡ºãªã—ã§ã—ãŸã€‚")
        return

    header = (
        f"â˜…â˜…â˜…â˜…â˜…ã€æŠ¼ã—ç›®ã€‘â˜…â˜…â˜…â˜…â˜… {now_jst().strftime('%m/%d %H:%M')}\n"
        f"æŠ½å‡º: {len(best_df)} éŠ˜æŸ„ï¼ˆé‡è¤‡çµ±åˆï¼‰\n"
        f"æ¡ä»¶: {REBOUND_MAX:.0f}%â‰¥åç™ºâ‰¥{REBOUND_MIN:.0f}%ãƒ»ä¸‹è½â‰¤{DROP_MAX:.0f}%ãƒ»SMA25ä¸Šãƒ»æœŸå¾…â‰¥{EXPECTED_RISE_MIN:.0f}%ãƒ»{DAYS_SINCE_MIN}æ—¥çµŒé\n"
        f"------------------------------"
    )
    send_long_text(header)

    for _, r in best_df.head(top_n).iterrows():
        ticker = str(r["Ticker"])
        name = ticker_name_map.get(ticker, "")
        upper  = r.get("Expected_Upper")
        latest = r.get("Latest_Close")
        low    = r.get("Pullback_Low")
        rise_p = r.get("Expected_Rise_%")
        prev   = r.get("Prev_Close")

        def fnum(x):
            try: return f"{float(x):,.0f}"
            except: return "-"
        def fpct(x):
            try: return f"{float(x):.1f}%"
            except: return "-"
        def fpct_signed(x):
            try:
                x = float(x)
                if not np.isfinite(x): return "-"
                return f"{x:+.1f}%"
            except:
                return "-"

        expect_amt = (float(upper) - float(latest)) if pd.notna(upper) and pd.notna(latest) else None
        chg_pct = ((float(latest) / float(prev)) - 1.0) * 100.0 if (pd.notna(latest) and pd.notna(prev) and float(prev) != 0.0) else None
        bot_pct = ((float(latest) / float(low)) - 1.0) * 100.0 if (pd.notna(latest) and pd.notna(low) and float(low) != 0.0) else None

        pull_date = r.get("Pullback_Date")
        pull_str = pull_date.strftime("%m/%d") if hasattr(pull_date, "strftime") else "-"
        rsi_val = latest_rsi_from_raw(raw_df, ticker, period=14)
        rsi_str = "-" if rsi_val is None or not np.isfinite(rsi_val) else f"{rsi_val:.0f}"

        # ãƒ†ã‚­ã‚¹ãƒˆ 5è¡Œï¼ˆcontentï¼‰
        line1 = f"{ticker} {name}".rstrip()
        line2 = f"åº•æ—¥ {pull_str}"
        line3 = f"â†— {fpct(rise_p)}   ğŸ¯ ä¸Š {fnum(upper)}   ä¸‹ {fnum(low)}"
        line4 = f"ä»Š {fnum(latest)}   ğŸ¯ æœŸå¾… {fnum(expect_amt)}  RSI {rsi_str}"
        line5 = f"å¤‰å‹•ç‡ {fpct_signed(chg_pct)}   åº•å€¤æ¯”è¼ƒ {fpct_signed(bot_pct)}"
        msg = "\n".join([line1, line2, line3, line4, line5])
        send_long_text(msg)

        # ãƒãƒ£ãƒ¼ãƒˆç”»åƒï¼ˆEmbedï¼‰
        img_path = save_chart_image_from_raw(raw_df, ticker, out_dir="charts")
        if img_path and PUBLIC_BASE_URL:
            public_url = f"{PUBLIC_BASE_URL}/{os.path.basename(img_path)}"
            title = f"{ticker} {name}".strip()
            desc = f"Window: best / æœŸå¾…ä¸Šæ˜‡ {fpct(rise_p)}"
            fields = [
                {"name": "Pullback", "value": f"{pull_str}", "inline": True},
                {"name": "Latest", "value": f"{fnum(latest)}", "inline": True},
                {"name": "Target", "value": f"{fnum(upper)}", "inline": True},
            ]
            discord_send_embed(title=title, description=desc, image_url=public_url, fields=fields)

def main():
    now = now_jst()
    force = os.getenv("FORCE_RUN") == "1"

    if not force and is_weekend(now):
        print(f"[SKIP] {now:%F %R} é€±æœ«ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—ï¼ˆFORCE_RUN=1ã§å®Ÿè¡Œå¯èƒ½ï¼‰")
        return

    best, raw, name_map = run_pipeline()
    notify(best, raw, name_map, top_n=TOP_N)

if __name__ == "__main__":
    main()


